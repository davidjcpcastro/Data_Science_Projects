{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a29b3aa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Index\n",
    "    \n",
    "[1. Install libraries](#1)<br>\n",
    "    \n",
    "[2. Import libraries](#2)<br>\n",
    "    \n",
    "[3. Check File Statistics](#3)<br>\n",
    "  \n",
    "[4. Deal with Missing Values](#4)<br>\n",
    "    \n",
    "[5. Check and Fix Inconsistencies](#5)<br>\n",
    "\n",
    "[6. Check and Delete Outliers](#6)<br>\n",
    "    \n",
    "[7. Feature Engeneering](#7)<br>\n",
    "\n",
    "[8. Feature Selection](#8)<br>\n",
    "\n",
    "[9. Data Normalization](#9)<br>\n",
    "    \n",
    "- [9.1 Min Max Scaler](#9.1)<br>\n",
    "    \n",
    "- [9.2 One Hot Encoding](#9.2)<br>  \n",
    "\n",
    "[10. Using DBScan to remove outliers](#10)<br>\n",
    "\n",
    "[11. Dimensionality Reduction: PCA](#11)<br>\n",
    "    \n",
    "[12. Models](#12)<br>\n",
    "    \n",
    "- [12.1 Hierarchical Clustering](#12.1)<br>\n",
    "    \n",
    "- [12.2 K Means Clustering](#12.2)<br>  \n",
    "\n",
    "- [12.3 Self-Organizing Maps](#12.3)<br>\n",
    "    - [12.3.1 K Means on top of SOM](#12.3.1)<br>  \n",
    "    - [12.3.2 Hierarchical Clustering on top of SOM](#12.3.2)<br>\n",
    "\n",
    "- [12.4 Birch Clustering](#12.4)<br>\n",
    "  \n",
    "- [12.5 Clustering by Perspectives](#12.5)<br>\n",
    "    - [12.5.1 K Means Clustering](#12.5.1)<br>  \n",
    "    - [12.5.2 Merging Perspectives](#12.5.2)<br>\n",
    "    \n",
    "[13. Cluster Visualization](#13)<br>\n",
    "    \n",
    "- [13.1 Using t-SNE](#13.1)<br>  \n",
    "    \n",
    "- [13.2 Using Polar Line Plot](#13.2)<br>\n",
    "    \n",
    "- [13.3 Using Cluster Profiles](#13.3)<br>\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a339fa5",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1\">\n",
    "\n",
    "# 1. Install libraries\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21551c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sas7bdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17abcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SOMPY-master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bed1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipdb==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e814c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92525ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eddcb54",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2\">\n",
    "\n",
    "# 2. Import libraries\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "from itertools import product\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "#For PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "#For Hierarcical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.cluster import Birch as Birch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import MeanShift, DBSCAN, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "#------ SOM-------\n",
    "from os.path import join\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "import sompy\n",
    "from sompy.visualization.mapview import View2D\n",
    "from sompy.visualization.bmuhits import BmuHitsView\n",
    "from sompy.visualization.hitmap import HitMapView\n",
    "from matplotlib.patches import RegularPolygon, Ellipse\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import cm, colorbar\n",
    "from matplotlib import colors as mpl_colors\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import __version__ as mplver\n",
    "#----- kmean-----\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "#-----hierarchical clustering----\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "from collections import Counter\n",
    "from os.path import join\n",
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "import graphviz\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347905b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sas7bdat import SAS7BDAT\n",
    "with SAS7BDAT(r'a2z_insurance.sas7bdat') as file:\n",
    "    df = file.to_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984ea349",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3\">\n",
    "\n",
    "# 3. Check File Statistics\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing missing values with numpy NaN\n",
    "df.replace(\"\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99484eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b174674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating metric features from non metric features\n",
    "non_metric_features = [\"EducDeg\", \"GeoLivArea\",\"Children\"]\n",
    "metric_features = df.columns.drop(non_metric_features).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "df_central = df.copy()\n",
    "df_central.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52709a6",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4\">\n",
    "\n",
    "# 4. Deal with Missing Values\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a6be7",
   "metadata": {},
   "source": [
    "Filling missing values with mode for non-metric features and median for metric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = df_central[non_metric_features].mode().loc[0]\n",
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f22b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = df_central[metric_features].median()\n",
    "medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebe27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_values = pd.concat([medians,modes]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_central.fillna(imp_values, inplace=True)\n",
    "df_central.isna().sum()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2824174",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5\">\n",
    "\n",
    "# 5. Check and Fix Inconsistencies\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_central[df_central.BirthYear < df_central.FirstPolYear]\n",
    "\n",
    "#There is inconsistency, however we decided to ignore it as later the birthYear will be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa00d3",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"6\">\n",
    "\n",
    "# 6. Check and Delete Outliers\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ffb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig, axes = plt.subplots(2, ceil(len(metric_features) / 2), figsize=(20, 11))\n",
    "for ax, feat in zip(axes.flatten(), metric_features): \n",
    "    sns.boxplot(x=df_central[feat], ax=ax)\n",
    "title = \"Figure 1: Numeric Variables' Box Plots with outliers\"\n",
    "plt.suptitle(title, size=30)\n",
    "plt.savefig('outliers1m.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c9fa9",
   "metadata": {},
   "source": [
    "The dataset contain many outliers, as it seen in FirstPolYear, BirthYear or PremHealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7014d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual filtering method\n",
    "filters1 = (\n",
    "    (df_central['FirstPolYear']<=2016)\n",
    "    &\n",
    "    (df_central['BirthYear']>=1916)\n",
    "    &\n",
    "    (df_central['ClaimsRate']<=100)\n",
    "    &\n",
    "    (df_central['MonthSal']<=20000)\n",
    "    &\n",
    "    (df_central['PremMotor']<=2000)\n",
    "    &\n",
    "    (df_central['PremHousehold']<=1400)\n",
    "    &\n",
    "    (df_central['PremHealth']<=5000)\n",
    "    &\n",
    "    (df_central['PremWork']<=250)\n",
    "    &\n",
    "    (df_central['CustMonVal']>=-2000)\n",
    "    &\n",
    "    (df_central['ClaimsRate']<=2)\n",
    "    &\n",
    "    (df_central['CustMonVal']<=1500)\n",
    "    &\n",
    "    (df_central['PremLife']<=250)\n",
    "    \n",
    ")\n",
    "\n",
    "df_1 = df_central[filters1]\n",
    "\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_1.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04debe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quartile method\n",
    "\n",
    "q25 = df_central.quantile(.25)\n",
    "q75 = df_central.quantile(.75)\n",
    "iqr = (q75 - q25)\n",
    "\n",
    "upper_lim = q75 + 1.5 * iqr\n",
    "lower_lim = q25 - 1.5 * iqr\n",
    "\n",
    "filters2 = []\n",
    "for metric in metric_features:\n",
    "    llim = lower_lim[metric]\n",
    "    ulim = upper_lim[metric]\n",
    "    filters2.append(df_central[metric].between(llim, ulim, inclusive='both'))\n",
    "\n",
    "filters2 = pd.Series(np.all(filters2, 0))\n",
    "df_2 = df_central[filters2]\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_2.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7764108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed method (quartile + filter)\n",
    "\n",
    "df_3 = df_central[(filters1 | filters2)]\n",
    "\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_3.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a875b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_central = df_3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa938b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig, axes = plt.subplots(2, ceil(len(metric_features) / 2), figsize=(20, 11))\n",
    "for ax, feat in zip(axes.flatten(), metric_features): \n",
    "    sns.boxplot(x=df_central[feat], ax=ax)\n",
    "title = \"Figure 2: Numeric Variables' Box Plots\"\n",
    "plt.suptitle(title, size=30)\n",
    "plt.savefig('outliers2m.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff90823",
   "metadata": {},
   "source": [
    "The dataset now look more balanced, as many less records are placed outise the quartile range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01113ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig, axes = plt.subplots(2, ceil(len(non_metric_features) / 2), figsize=(20, 11))\n",
    "for ax, feat in zip(axes.flatten(), non_metric_features): \n",
    "    ax.hist(df_central[feat])\n",
    "    ax.set_title(feat)\n",
    "title = \"Non Metric Variables' Histograms\"\n",
    "plt.suptitle(title)\n",
    "plt.savefig('outliers1nm.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347d9f6",
   "metadata": {},
   "source": [
    "The non_metric features do not containing any visible outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e141358",
   "metadata": {},
   "source": [
    "\n",
    "<a class=\"anchor\" id=\"7\">\n",
    "\n",
    "# 7. Feature Engeneering\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe6dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new features called birthyear based on age\n",
    "df_central['Age'] = 2016 - df_central['BirthYear']\n",
    "metric_features.append('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new features for the number of years since first policy happen\n",
    "df_central['PolicyYears'] = 2016 - df_central['FirstPolYear']\n",
    "metric_features.append('PolicyYears')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new feature resulting from the sum of all premiums\n",
    "df_central['TotalPremiums'] = df_central['PremLife'] + df_central['PremHousehold'] + df_central['PremHealth'] + df_central['PremWork'] + df_central['PremMotor']\n",
    "metric_features.append('TotalPremiums')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d282ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new features called yearly salary based on the month salary\n",
    "df_central['YearSal'] = df_central['MonthSal']*12\n",
    "metric_features.append('YearSal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new features to represent what percentage of the yearly salary is spent on premiums\n",
    "df_central['Premiums%Salary'] = df_central['TotalPremiums'] / df_central['YearSal']\n",
    "metric_features.append('Premiums%Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766a9a1",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"8\">\n",
    "\n",
    "# 8. Feature Selection\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acdd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "corr = np.round(df_central[metric_features].corr(method=\"pearson\"), decimals=2)\n",
    "mask_annot = np.absolute(corr.values) >= 0.5\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\"))\n",
    "sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True), \n",
    "            fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
    "fig.subplots_adjust(top=0.95)\n",
    "fig.suptitle(\"Figure 3: Correlation Matrix\", fontsize=20)\n",
    "plt.savefig('CorMtx1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ef3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove BirthYear and MonthSal features\n",
    "df_central.drop('BirthYear', inplace=True, axis=1)\n",
    "metric_features.remove('BirthYear')\n",
    "df_central.drop('MonthSal', inplace=True, axis=1)\n",
    "metric_features.remove('MonthSal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b54fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Age feature\n",
    "df_central.drop('Age',inplace=True,axis=1)\n",
    "metric_features.remove('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93afdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove PolicyYear and FirstPolYear features\n",
    "df_central.drop('PolicyYears',inplace=True,axis=1)\n",
    "metric_features.remove('PolicyYears')\n",
    "df_central.drop('FirstPolYear',inplace=True,axis=1)\n",
    "metric_features.remove('FirstPolYear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove CustID feature\n",
    "df_central.drop('CustID',inplace=True,axis=1)\n",
    "metric_features.remove('CustID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5797e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove ClaimsRate feature\n",
    "df_central.drop('ClaimsRate',inplace=True,axis=1)\n",
    "metric_features.remove('ClaimsRate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Matrix after the features selection\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "corr = np.round(df_central[metric_features].corr(method=\"pearson\"), decimals=2)\n",
    "mask_annot = np.absolute(corr.values) >= 0.5\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\")) \n",
    "sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True), \n",
    "            fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
    "fig.subplots_adjust(top=0.95)\n",
    "fig.suptitle(\"Correlation Matrix\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64fbb77",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"9\">\n",
    "\n",
    "# 9. Data Normalization\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf09c8d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"9.1\">\n",
    "\n",
    "## 9.1. Min Max Scaler\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax = df_central.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a62548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MinMaxScaler to scale the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_feat = scaler.fit_transform(df_minmax[metric_features])\n",
    "scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fcdd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax[metric_features] = scaled_feat\n",
    "df_minmax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax[metric_features].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec970f1",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"9.2\">\n",
    "\n",
    "## 9.2. One Hot Encoding\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohc = df_minmax.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe48adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical features. Get feature names and create a DataFrame \n",
    "\n",
    "ohc = OneHotEncoder(sparse=False, drop=\"first\")\n",
    "ohc_feat = ohc.fit_transform(df_ohc[non_metric_features])\n",
    "ohc_feat_names = ohc.get_feature_names_out()\n",
    "ohc_df = pd.DataFrame(ohc_feat, index=df_ohc.index, columns=ohc_feat_names)\n",
    "ohc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain ohc variables\n",
    "df_ohc = pd.concat([df_ohc.drop(columns=non_metric_features), ohc_df], axis=1)\n",
    "df_ohc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3178d6",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"10\">\n",
    "\n",
    "# 10. Using DBScan to remove outliers\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN method to check for outliers\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=30, n_jobs=4)\n",
    "dbscan_labels = dbscan.fit_predict(df_ohc[metric_features])\n",
    "Counter(dbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outliers in a dataframe to analyse them later\n",
    "df_outliers_dbscan = df_ohc[dbscan_labels==-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New df without outliers\n",
    "df_before_models = df_ohc[dbscan_labels!=-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b57a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Box Plots after removing the outliers with dbscan\n",
    "sns.set()\n",
    "fig, axes = plt.subplots(2, ceil(len(metric_features) / 2), figsize=(20, 11))\n",
    "for ax, feat in zip(axes.flatten(), metric_features): \n",
    "    sns.boxplot(x=df_before_models[feat], ax=ax)         \n",
    "title = \"Numeric Variables' Box Plots\"\n",
    "plt.suptitle(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19dffa",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"11\">\n",
    "\n",
    "# 11. Dimensionality Reduction: PCA\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca=df_before_models.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e16c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca[metric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: STANDARDIZATION - already done with the normalization with Min Max Scaler to the continuous variables [metric_]\n",
    "#STEP 2: COVARIANCE MATRIX COMPUTATION\n",
    "\n",
    "pca = PCA()\n",
    "pca_feat =  pca.fit_transform(df_pca[metric_features])\n",
    "pca_feat.shape  # What is this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output PCA table\n",
    "# STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS\n",
    "pd.DataFrame(\n",
    "    {\"Eigenvalue\": pca.explained_variance_,\n",
    "     \"Difference\": np.insert(np.diff(pca.explained_variance_), 0, 0),\n",
    "     \"Proportion\": pca.explained_variance_ratio_,\n",
    "     \"Cumulative\": np.cumsum(pca.explained_variance_ratio_)},\n",
    "    index=range(1, pca.n_components_ + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure and axes\n",
    "fig,(ax1, ax2) = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "# draw plots\n",
    "ax1.plot(pca.explained_variance_, marker=\".\", markersize=12)  # CODE HERE: PLOT THE EIGENVALUES (EXPLAINED VARIANCE)\n",
    "ax2.plot(pca.explained_variance_ratio_, marker=\".\", markersize=12, label=\"Proportion\")  # CODE HERE: PLOT THE EXPLAINED VARIANCE RATIO\n",
    "ax2.plot(np.cumsum(pca.explained_variance_ratio_), marker=\".\", markersize=12, linestyle=\"--\", label=\"Cumulative\")  # CODE HERE: PLOT THE CUMULATIVE EXPLAINED VARIANCE RATIO\n",
    "\n",
    "# customizations\n",
    "ax2.legend()\n",
    "ax1.set_title(\"Scree Plot\", fontsize=14)\n",
    "ax2.set_title(\"Variance Explained\", fontsize=14)\n",
    "ax1.set_ylabel(\"Eigenvalue\")\n",
    "ax2.set_ylabel(\"Proportion\")\n",
    "ax1.set_xlabel(\"Components\")\n",
    "ax2.set_xlabel(\"Components\")\n",
    "ax1.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax1.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "ax2.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax2.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a15647",
   "metadata": {},
   "source": [
    "From these graphics we see that: the 1st PC is very important with almost 50% of all data; and with 5 components we only loose around 6% of the data. Let´s try then with 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf15b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA again with the number of principal components you want to retain\n",
    "# STEP 4: FEATURE VECTOR\n",
    "pca = PCA(n_components=6)\n",
    "pca_feat = pca.fit_transform(df_pca[metric_features])\n",
    "pca_feat_names = [f\"PC{i}\" for i in range(pca.n_components_)]\n",
    "pca_df = pd.DataFrame(pca_feat, index=df_pca.index, columns=pca_feat_names)  # remember index=df_pca.index\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4401bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain pca variables\n",
    "# LAST STEP: RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES\n",
    "df_pca = pd.concat([df_pca, pca_df], axis=1)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cdec58",
   "metadata": {},
   "source": [
    "Let´s interpret each Principal Component (with style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _color_red_or_green(val):\n",
    "    if val < -0.45:\n",
    "        color = 'background-color: red'\n",
    "    elif val > 0.45:\n",
    "        color = 'background-color: green'\n",
    "    else:\n",
    "        color = ''\n",
    "    return color\n",
    "\n",
    "# Interpreting each Principal Component\n",
    "# CODE HERE: Obtain the loadings (i.e. correlation between PCs and original features)\n",
    "loadings = df_pca[metric_features + pca_feat_names].corr().loc[metric_features, pca_feat_names]\n",
    "loadings.style.applymap(_color_red_or_green)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332e492",
   "metadata": {},
   "source": [
    "From this colorful table we can conclude that we can eliminate PC4 and PC5 as their highest  correlations are already taking in a count in PC0 and PC3, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.drop(columns=['PC4','PC5'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c27b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(\n",
    "    df_pca,\n",
    "    title='A2Z Insurance Data Preprocessed',\n",
    "    correlations={\n",
    "        \"pearson\": {\"calculate\": True},\n",
    "        \"spearman\": {\"calculate\": False},\n",
    "        \"kendall\": {\"calculate\": False},\n",
    "        \"phi_k\": {\"calculate\": False},\n",
    "        \"cramers\": {\"calculate\": False},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da2164c",
   "metadata": {},
   "source": [
    "Now just plot a graphic between PC0 and PC1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18830cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing in 2D for PC0 and PC1:\n",
    "fig = px.scatter(df_pca, x=\"PC0\", y=\"PC1\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D:\n",
    "fig = px.scatter_3d(\n",
    "    df_pca, x=\"PC0\", y=\"PC1\", z=\"PC2\",\n",
    "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1564f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"PC0\", \"PC1\",\"PC2\",\"PC3\"]\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    df_pca,\n",
    "    dimensions=features\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c04eb",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12\">\n",
    "\n",
    "# 12. Models\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0b247",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12.1\">\n",
    "\n",
    "## 12.1. Hierarchical Clustering\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656f27f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hc = df_before_models.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753994e",
   "metadata": {},
   "source": [
    "Defining the linkage method to choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b60c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2_hc(df, link_method, max_nclus, min_nclus=1, dist=\"euclidean\"):\n",
    "    \"\"\"This function computes the R2 for a set of cluster solutions given by the application of a hierarchical method.\n",
    "    The R2 is a measure of the homogenity of a cluster solution. It is based on SSt = SSw + SSb and R2 = SSb/SSt. \n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to apply clustering\n",
    "    link_method (str): either \"ward\", \"complete\", \"average\", \"single\"\n",
    "    max_nclus (int): maximum number of clusters to compare the methods\n",
    "    min_nclus (int): minimum number of clusters to compare the methods. Defaults to 1.\n",
    "    dist (str): distance to use to compute the clustering solution. Must be a valid distance. Defaults to \"euclidean\".\n",
    "    \n",
    "    Returns:\n",
    "    ndarray: R2 values for the range of cluster solutions\n",
    "    \"\"\"\n",
    "    def get_ss(df):\n",
    "        ss = np.sum(df.var() * (df.count() - 1))\n",
    "        return ss  # return sum of sum of squares of each df variable\n",
    "    sst = get_ss(df)  # get total sum of squares\n",
    "    r2 = []  # where we will store the R2 metrics for each cluster solution\n",
    "    for i in range(min_nclus, max_nclus+1):  # iterate over desired ncluster range\n",
    "        cluster = AgglomerativeClustering(n_clusters=i, affinity=dist, linkage=link_method)\n",
    "        hclabels = cluster.fit_predict(df) #get cluster labels\n",
    "        df_concat = pd.concat((df, pd.Series(hclabels, name='labels')), axis=1)  # concat df with labels\n",
    "        ssw_labels = df_concat.groupby(by='labels').apply(get_ss)  # compute ssw for each cluster labels\n",
    "        ssb = sst - np.sum(ssw_labels)  # remember: SST = SSW + SSB\n",
    "        r2.append(ssb / sst)  # save the R2 of the given cluster solution\n",
    "    return np.array(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281dbaf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_r2_plot_hierarchical(df_hc,hc_methods,distance):\n",
    "    # Call function defined above to obtain the R2 statistic for each hc_method\n",
    "    max_nclus = 10\n",
    "    r2_hc_methods = np.vstack(\n",
    "        [\n",
    "            get_r2_hc(df=df_hc[metric_features], link_method=link, max_nclus=max_nclus,dist = distance) \n",
    "            for link in hc_methods\n",
    "        ]\n",
    "    ).T\n",
    "    r2_hc_methods = pd.DataFrame(r2_hc_methods, index=range(1, max_nclus + 1), columns=hc_methods)\n",
    "\n",
    "    sns.set()\n",
    "    # Plot data\n",
    "    fig = plt.figure(figsize=(11,5))\n",
    "    sns.lineplot(data=r2_hc_methods, linewidth=2.5, markers=[\"o\"]*len(hc_methods))\n",
    "\n",
    "    # Finalize the plot\n",
    "    fig.suptitle(\"R2 plot for various hierarchical methods\", fontsize=21)\n",
    "    plt.gca().invert_xaxis()  # invert x axis\n",
    "    plt.legend(title=\"HC methods\", title_fontsize=11)\n",
    "    plt.xticks(range(1, max_nclus + 1))\n",
    "    plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "    plt.ylabel(\"R2 metric\", fontsize=13)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "distance = 'euclidean'\n",
    "draw_r2_plot_hierarchical(df_hc,methods,distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5610f5ff",
   "metadata": {},
   "source": [
    "For the Euclidean distance: We conclude that from 1 to 5 clusters, the best method is the Complete one\n",
    "\n",
    "Then, for 6 clusters is the Single one\n",
    "\n",
    "And after that is the Ward one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d67a1",
   "metadata": {},
   "source": [
    "Now let´s test for the Manhattan distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a43c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"complete\", \"average\", \"single\"]\n",
    "distance = 'manhattan'\n",
    "draw_r2_plot_hierarchical(df_hc,methods,distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24dbd60",
   "metadata": {},
   "source": [
    "The Manhattan distance is not as good as the Euclidean. So let´s keep the euclidean one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa529a5",
   "metadata": {},
   "source": [
    "Defining the number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage = 'complete'\n",
    "distance = 'euclidean'\n",
    "hclust = AgglomerativeClustering(linkage=linkage, affinity=distance, distance_threshold=0, n_clusters=None)\n",
    "hclust.fit_predict(df_hc[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043693b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "def draw_dendrogram(y_threshold,hclust):\n",
    "    # create the counts of samples under each node (number of points being merged)\n",
    "    counts = np.zeros(hclust.children_.shape[0])\n",
    "    n_samples = len(hclust.labels_)\n",
    "\n",
    "    # hclust.children_ contains the observation ids that are being merged together\n",
    "    # At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "    for i, merge in enumerate(hclust.children_):\n",
    "        # track the number of observations in the current cluster being formed\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                # If this is True, then we are merging an observation\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                # Otherwise, we are merging a previously formed cluster\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    # the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "    # the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "    # the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [hclust.children_, hclust.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    sns.set()\n",
    "    fig = plt.figure(figsize=(11,5))\n",
    "    # The Dendrogram parameters need to be tuned\n",
    "    dendrogram(linkage_matrix, truncate_mode='level', p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "    plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "    plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "    plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "    plt.ylabel(f'{distance.title()} Distance', fontsize=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dendrogram(1.5,hclust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d731c",
   "metadata": {},
   "source": [
    "Final Hierarchical clustering solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 cluster solution\n",
    "linkage = 'complete'\n",
    "distance = 'euclidean'\n",
    "hc4lust = AgglomerativeClustering(linkage=linkage, affinity=distance, n_clusters=4)\n",
    "hc4_labels = hc4lust.fit_predict(df_hc[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the 4 clusters\n",
    "df_final_hc = pd.concat((df_hc, pd.Series(hc4_labels, name='labels')), axis=1)\n",
    "df_final_hc.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using R²\n",
    "def get_ss(df):\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable\n",
    "\n",
    "def get_r2(df):\n",
    "    sst = get_ss(df[metric_features])  # get total sum of squares\n",
    "    ssw_labels = df[metric_features + [\"labels\"]].groupby(by='labels').apply(get_ss)  # compute ssw for each cluster labels\n",
    "    ssb = sst - np.sum(ssw_labels)  # remember: SST = SSW + SSB\n",
    "    r2 = ssb / sst\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a61d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_r2(df_final_hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3456f889",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12.2\">\n",
    "\n",
    "## 12.2. K Means Clustering\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans = df_before_models.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4d35e",
   "metadata": {},
   "source": [
    "Defining the number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c85836",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_clusters = range(2, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1376c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for n_clus in r_clusters: \n",
    "    kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=15, random_state=1)\n",
    "    kmclust.fit(df_kmeans[metric_features])\n",
    "    inertia.append(kmclust.inertia_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_inertia_plot(range_clusters,inertia_values):\n",
    "    # The inertia plot\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.plot(range_clusters,inertia_values)\n",
    "    plt.ylabel(\"Inertia: SSw\")\n",
    "    plt.xlabel(\"Number of clusters\")\n",
    "    plt.title(\"Inertia plot over clusters\", size=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf3a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_inertia_plot(r_clusters,inertia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c631de",
   "metadata": {},
   "source": [
    "From this graphic and using the elbow method, the best nº of clusters is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5864d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "\n",
    "def draw_silhouette_plot(range_clusters,df):\n",
    "    avg_silhouette = []\n",
    "    for nclus in range_clusters:\n",
    "        # Skip nclus == 1\n",
    "        if nclus == 1:\n",
    "            continue\n",
    "        # Create a figure\n",
    "        fig = plt.figure(figsize=(13, 7))\n",
    "        # Initialize the KMeans object with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        kmclust = KMeans(n_clusters=nclus, init='k-means++', n_init=15, random_state=1)\n",
    "        cluster_labels = kmclust.fit_predict(df[metric_features])\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed clusters\n",
    "        silhouette_avg = silhouette_score(df[metric_features], cluster_labels)\n",
    "        avg_silhouette.append(silhouette_avg)\n",
    "        print(f\"For n_clusters = {nclus}, the average silhouette_score is : {silhouette_avg}\")\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(df[metric_features], cluster_labels)\n",
    "        y_lower = 10\n",
    "        for i in range(nclus):\n",
    "            # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "            # Get y_upper to demarcate silhouette y range size\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "            # Filling the silhouette\n",
    "            color = cm.nipy_spectral(float(i) / nclus)\n",
    "            plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "\n",
    "        plt.title(\"The silhouette plot for the various clusters.\")\n",
    "        plt.xlabel(\"The silhouette coefficient values\")\n",
    "        plt.ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        # The silhouette coefficient can range from -1, 1\n",
    "        xmin, xmax = np.round(sample_silhouette_values.min() -0.1, 2), np.round(sample_silhouette_values.max() + 0.1, 2)\n",
    "        plt.xlim([xmin, xmax])\n",
    "        plt.ylim([0, len(df[metric_features]) + (nclus + 1) * 10])\n",
    "        plt.yticks([]) \n",
    "        plt.xticks(np.arange(xmin, xmax, 0.1))\n",
    "    return avg_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133a5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_silhouette = draw_silhouette_plot(r_clusters,df_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### AVERAGE SILHOUETTE PLOT #############\n",
    "\n",
    "def draw_average_silhouette(range_clusters,avg_silhouette):\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.plot(range_clusters,avg_silhouette)\n",
    "    plt.ylabel(\"Average silhouette\")\n",
    "    plt.xlabel(\"Number of clusters\")\n",
    "    plt.title(\"Average silhouette plot over clusters\", size=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_average_silhouette(r_clusters,average_silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4c27b",
   "metadata": {},
   "source": [
    "After seeing this graphic, since the average silhouette for 3 and 4 clusters is very similar the best choice might be 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final cluster solution\n",
    "number_clusters = 3\n",
    "kmclust = KMeans(n_clusters=number_clusters, init='k-means++', n_init=15, random_state=1)\n",
    "km_labels = kmclust.fit_predict(df_kmeans[metric_features])\n",
    "km_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b32f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Characterizing the final clusters\n",
    "df_final_kmeans = pd.concat((df_kmeans, pd.Series(km_labels, name='labels')), axis=1)\n",
    "df_final_kmeans.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a475c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_r2(df_final_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402faf72",
   "metadata": {},
   "source": [
    "Final R square value for K means is 0.0373747"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab6878",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12.3\">\n",
    "\n",
    "## 12.3. Self-Organizing Maps\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_som = df_before_models.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse initially with a lower mapsize of 10x10\n",
    "np.random.seed(42)\n",
    "\n",
    "sm = sompy.SOMFactory().build(\n",
    "    df_som[metric_features].values, \n",
    "    mapsize=[10, 10], \n",
    "    initialization='random', \n",
    "    neighborhood='gaussian',\n",
    "    training='batch',\n",
    "    lattice='hexa',\n",
    "    component_names=metric_features\n",
    ")\n",
    "sm.train(n_job=4, verbose='info', train_rough_len=100, train_finetune_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 72\n",
    "weights = sm.codebook.matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53934c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### Visualizing Component Planes ##\n",
    "###################################\n",
    "\n",
    "def plot_component_planes(weights,\n",
    "                          features,\n",
    "                          M=3, N=4, \n",
    "                          figsize=(20,20),\n",
    "                          figlayout=(3,4),\n",
    "                          title=\"Component Planes\",\n",
    "                          cmap=cm.magma\n",
    "                         ):\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(M), np.arange(N))\n",
    "    xx = xx.astype(float)\n",
    "    yy = yy.astype(float)\n",
    "    xx[::-2] -= 0.5\n",
    "    xx = xx.T\n",
    "    yy = yy.T\n",
    "    weights_ = weights.reshape((M,N,len(features)))\n",
    "    fig = plt.figure(figsize=figsize, constrained_layout=True)\n",
    "    subfigs = fig.subfigures(figlayout[0], figlayout[1], wspace=.15)\n",
    "    ## Normalize color scale to range of all values\n",
    "    colornorm = mpl_colors.Normalize(vmin=np.min(weights), \n",
    "                                         vmax=np.max(weights))\n",
    "    for cpi, sf in zip(range(len(metric_features)), subfigs.flatten()):\n",
    "\n",
    "        sf.suptitle(features[cpi], y=0.95)\n",
    "        axs = sf.subplots(1,1, )\n",
    "        axs.set_aspect('equal')\n",
    "\n",
    "        ## Normalize color scale to range of values in each component\n",
    "        colornorm = mpl_colors.Normalize(vmin=np.min(weights_[:,:,cpi]), \n",
    "                                         vmax=np.max(weights_[:,:,cpi]))\n",
    "\n",
    "        # iteratively add hexagons\n",
    "        for i in range(weights_.shape[0]):\n",
    "            for j in range(weights_.shape[1]):\n",
    "                wy = yy[(i, j)] * np.sqrt(3) / 2\n",
    "                hexagon = RegularPolygon((xx[(i, j)], wy), \n",
    "                                     numVertices=6, \n",
    "                                     radius=.99 / np.sqrt(3),\n",
    "                                     facecolor=cmap(colornorm(weights_[i, j, cpi])), \n",
    "                                     alpha=1, \n",
    "                                     linewidth=.5,\n",
    "                                     edgecolor=cmap(colornorm(weights_[i, j, cpi]))\n",
    "                                    )\n",
    "                axs.add_patch(hexagon)\n",
    "\n",
    "        ## only run this block if matplotlib >= 3.6.x\n",
    "        mplv = [int(i) for i in mplver.split('.')]\n",
    "        if mplv[1] >= 6:\n",
    "            ## Add colorbar\n",
    "            divider = make_axes_locatable(axs)\n",
    "            ax_cb = divider.append_axes(\"right\", size=\"7%\")#, pad=\"2%\")\n",
    "\n",
    "            ## Create a Mappable object\n",
    "            cmap_sm = plt.cm.ScalarMappable(cmap=cmap, norm=colornorm)\n",
    "            cmap_sm.set_array([])\n",
    "\n",
    "            ## Create custom colorbar \n",
    "            cb1 = colorbar.Colorbar(ax_cb,\n",
    "                                    orientation='vertical', \n",
    "                                    alpha=1,\n",
    "                                    mappable=cmap_sm\n",
    "                                    )\n",
    "            cb1.ax.get_yaxis().labelpad = 16\n",
    "\n",
    "            ## Add colorbar to plot\n",
    "            sf.add_axes(ax_cb)\n",
    "\n",
    "        ## Remove axes for hex plot\n",
    "        axs.margins(.05)\n",
    "        axs.axis(\"off\")\n",
    "    fig.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ebd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"matplotlib version is:\" , mplver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea799f",
   "metadata": {},
   "source": [
    "## Component Planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa71c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Run this cell ONLY if matplotlib version is <= 3.4.x\n",
    "# ## Comment out this cell and run the cell below instead if it doesn't work.\n",
    "\n",
    "\n",
    "# # Visualizing the Component planes (feature values)\n",
    "sns.set()\n",
    "view2D = View2D(12, 12, \"\", text_size=10)\n",
    "view2D.show(sm, col_sz=3, what='codebook')\n",
    "plt.subplots_adjust(top=0.90)\n",
    "plt.suptitle(\"Component Planes\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Run this cell ONLY if matplotlib version is > 3.4.x\n",
    "#plot_component_planes(weights=sm.codebook.matrix,features=metric_features,\n",
    "#                     M=10,N=10,\n",
    "#                     figsize=(12,15),figlayout=(4,3),\n",
    "#                     title=\"Component Planes\",\n",
    "#                     cmap=sns.color_palette(\"rocket\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a77b22",
   "metadata": {},
   "source": [
    "## U-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ba49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you have U-matrix\n",
    "u = sompy.umatrix.UMatrixView(9, 9, 'umatrix', show_axis=True, text_size=8, show_text=True)\n",
    "\n",
    "UMAT = u.show(\n",
    "    sm, \n",
    "    distance=2,\n",
    "    row_normalized=False,\n",
    "    show_data=True, \n",
    "    contour=True, # Visualize isomorphic curves\n",
    "    blob=True\n",
    ")\n",
    "\n",
    "np.flip(UMAT[1], axis=1) # U-matrix values - they match with the plot colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f138a84",
   "metadata": {},
   "source": [
    "## Hit-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39459ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vhts  = BmuHitsView(12,12,\"Hits Map\")\n",
    "vhts.show(sm, anotate=True, onlyzeros=False, labelsize=12, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c2623",
   "metadata": {},
   "source": [
    "## Unfolding phase\n",
    "### Clustering with SOMs: K-means SOM vs Emergent SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbb771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Repeat the analisy with an higher map size of 50 x 50\n",
    "np.random.seed(42)\n",
    "\n",
    "sm = sompy.SOMFactory().build(\n",
    "    df_som[metric_features].values, \n",
    "    mapsize=[50, 50],  \n",
    "    initialization='random',\n",
    "    neighborhood='gaussian',\n",
    "    training='batch',\n",
    "    lattice='hexa',\n",
    "    component_names=metric_features\n",
    ")\n",
    "sm.train(n_job=-1, verbose='info', train_rough_len=100, train_finetune_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6fc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### MATPLOTLIB VERSON <= 3.4 ##############\n",
    "sns.set()\n",
    "view2D = View2D(12, 12, \"\", text_size=10)\n",
    "view2D.show(sm, col_sz=3, what='codebook')\n",
    "plt.subplots_adjust(top=0.90)\n",
    "plt.suptitle(\"Component Planes\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_component_planes(weights=sm.codebook.matrix,features=metric_features,\n",
    "#                     M=50,N=50, # change to 50 50 if chnaged in codeline before\n",
    "#                     figsize=(12,15),figlayout=(4,3),\n",
    "#                     title=\"Component Planes 50x50\",\n",
    "#                     cmap=sns.color_palette(\"rocket\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-matrix of the 50x50 grid\n",
    "u = sompy.umatrix.UMatrixView(9, 9, 'umatrix', show_axis=True, text_size=8, show_text=True)\n",
    "\n",
    "UMAT = u.show(\n",
    "    sm, \n",
    "    distance=2, \n",
    "    row_normalized=False, \n",
    "    show_data=False, \n",
    "    contour=True # Visualize isomorphic curves\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78af1383",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12.3.1\">\n",
    "\n",
    "### 12.3.1. K Means on top of SOM\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea4bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_somK = df_som.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f2b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_clusters = range(2, 11)\n",
    "inertia = []\n",
    "for n_clus in r_clusters:  \n",
    "    kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=15, random_state=1)\n",
    "    kmclust.fit(df_somK[metric_features])\n",
    "    inertia.append(kmclust.inertia_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_inertia_plot(r_clusters,inertia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ef276",
   "metadata": {},
   "source": [
    "The number of cluster fo k-Means is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d2229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_s = draw_silhouette_plot(r_clusters,df_somK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bcf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_average_silhouette(r_clusters,average_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461353fd",
   "metadata": {},
   "source": [
    "we select 3 clusters as the difference with 4 is very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7101ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on top of the 2500 untis (sm.get_node_vectors() output)\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=20, random_state=42)\n",
    "nodeclus_labels = kmeans.fit_predict(sm.codebook.matrix)\n",
    "sm.cluster_labels = nodeclus_labels  # setting the cluster labels of sompy\n",
    "\n",
    "hits = HitMapView(12, 12,\"Clustering\", text_size=10)\n",
    "hits.show(sm, anotate=True, onlyzeros=False, labelsize=7, cmap=\"Pastel1\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the nodes and and respective clusters\n",
    "nodes = sm.codebook.matrix\n",
    "\n",
    "df_nodes = pd.DataFrame(nodes, columns=metric_features)\n",
    "df_nodes['labels'] = nodeclus_labels\n",
    "df_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8047682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining SOM's BMUs labels\n",
    "bmus_map = sm.find_bmu(df_somK[metric_features])[0]  # get bmus for each observation in df\n",
    "\n",
    "df_bmus = pd.DataFrame(\n",
    "    np.concatenate((df_somK, np.expand_dims(bmus_map,1)), axis=1),\n",
    "    index=df_somK.index, columns=np.append(df_somK.columns,\"BMU\")\n",
    ")\n",
    "df_bmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63660d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster labels for each observation\n",
    "df_final_somK = df_bmus.merge(df_nodes['labels'], 'left', left_on=\"BMU\", right_index=True)\n",
    "df_final_somK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1121ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the final clusters\n",
    "df_final_somK.drop(columns='BMU').groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125908bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_r2(df_final_somK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0770d5",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12.3.2\">\n",
    "\n",
    "### 12.3.2 Hierarchical Clustering on top of SOM\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_somH = df_som.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "distance = 'euclidean'\n",
    "draw_r2_plot_hierarchical(df_somH,methods,distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d900ddc",
   "metadata": {},
   "source": [
    "we still use ward as is the best for higher clusters after 7 and is quite similar to the others <7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc75c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting distance_threshold=0 and n_clusters=None ensures we compute the full tree\n",
    "linkage = 'ward'\n",
    "distance = 'euclidean'\n",
    "hclust = AgglomerativeClustering(linkage=linkage, affinity=distance, distance_threshold=0, n_clusters=None)\n",
    "hclust.fit_predict(df_somH[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d51a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dendrogram(15,hclust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf98355",
   "metadata": {},
   "source": [
    "select 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a67b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform Hierarchical clustering on top of the 2500 untis (sm.get_node_vectors() output)\n",
    "hierclust = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "nodeclus_labels = hierclust.fit_predict(sm.codebook.matrix)\n",
    "sm.cluster_labels = nodeclus_labels  # setting the cluster labels of sompy\n",
    "\n",
    "hits  = HitMapView(12, 12,\"Clustering\",text_size=10)\n",
    "hits.show(sm, anotate=True, onlyzeros=False, labelsize=7, cmap=\"Pastel1\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the nodes and and respective clusters\n",
    "nodes_H = sm.codebook.matrix\n",
    "\n",
    "df_nodes_H = pd.DataFrame(nodes_H, columns=metric_features)\n",
    "df_nodes_H['labels'] = nodeclus_labels\n",
    "df_nodes_H "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be878a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining SOM's BMUs labels\n",
    "bmus_map_H = sm.find_bmu(df_somH[metric_features])[0]  # get bmus for each observation in df\n",
    "\n",
    "df_bmus_H = pd.DataFrame(\n",
    "    np.concatenate((df_somH, np.expand_dims(bmus_map_H,1)), axis=1),\n",
    "    index=df_somH.index, columns=np.append(df_somH.columns,\"BMU\")\n",
    ")\n",
    "df_bmus_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ee28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster labels for each observation\n",
    "df_final_somH = df_bmus_H.merge(df_nodes_H['labels'], 'left', left_on=\"BMU\", right_index=True)\n",
    "df_final_somH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d7ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the final clusters\n",
    "df_final_somH.drop(columns='BMU').groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_r2(df_final_somH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1055ace",
   "metadata": {},
   "source": [
    "### Final SOM Clustering solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea600f",
   "metadata": {},
   "source": [
    "The r2 for kmeans on top of SOM (0.2332850987604802) is better then the Hierarchical Clustering on top of SOM units (0.20772534557034383)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cae3e2",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12.4\">\n",
    "\n",
    "## 12.4. Birch Clustering\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df):\n",
    "    \"\"\"Computes the sum of squares for all variables given a dataset\n",
    "    \"\"\"\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable\n",
    "\n",
    "def r2(df, labels):\n",
    "    sst = get_ss(df)\n",
    "    ssw = np.sum(df.groupby(labels).apply(get_ss))\n",
    "    return 1 - ssw/sst\n",
    "    \n",
    "def get_r2_scores(df, clusterer, min_k=2, max_k=10):\n",
    "    \"\"\"\n",
    "    Loop over different values of k. To be used with sklearn clusterers.\n",
    "    \"\"\"\n",
    "    r2_clust = {}\n",
    "    for n in range(min_k, max_k):\n",
    "        clust = clone(clusterer).set_params(n_clusters=n)\n",
    "        labels = clust.fit_predict(df)\n",
    "        r2_clust[n] = r2(df, labels)\n",
    "    return r2_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f1f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_birch = df_before_models.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8eb6ef",
   "metadata": {},
   "source": [
    "## Optimal Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_model = Birch()\n",
    "r2_birch_score = {}\n",
    "r2_birch_score['birch'] = get_r2_scores(df_birch,birch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_birch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311ef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(r2_birch_score).plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Birch Clustering:\\nR² plot for various number of clusters\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster method\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R² metric\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baccd6c",
   "metadata": {},
   "source": [
    "## Implementing Birch Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Cluster Solution\n",
    "birch_model = Birch(n_clusters = 4,threshold = 0.4)\n",
    "birch_labels = birch_model.fit_predict(df_birch[metric_features])\n",
    "birch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c514a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the final clusters\n",
    "df_final_birch = pd.concat((df_birch,pd.Series(birch_labels,name = 'labels')),axis=1)\n",
    "df_final_birch.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c281ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_r2(df_final_birch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da68c2a",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12.5\">\n",
    "\n",
    "## 12.5. Clustering by Perspectives\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94867b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before_models[metric_features].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "services_features = [\n",
    "    'PremMotor',\n",
    "    'PremHousehold',\n",
    "    'PremHealth',\n",
    "    'PremLife',\n",
    "    'PremWork',\n",
    "    'TotalPremiums'\n",
    "]\n",
    "\n",
    "customer_features = [\n",
    "    'CustMonVal', \n",
    "    'YearSal', \n",
    "    'Premiums%Salary'\n",
    "]\n",
    "\n",
    "df_services = df_before_models[services_features].copy()\n",
    "df_customer = df_before_models[customer_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hierarchical = AgglomerativeClustering(\n",
    "    affinity='euclidean'\n",
    ")\n",
    "\n",
    "birch = Birch(\n",
    "    n_clusters=2, threshold =0.1\n",
    ")\n",
    "\n",
    "\n",
    "spectral = SpectralClustering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4747c",
   "metadata": {},
   "source": [
    "## Services Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c253dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on services variables\n",
    "r2_scores_services = {}\n",
    "r2_scores_services['kmeans'] = get_r2_scores(df_services, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores_services[linkage] = get_r2_scores(\n",
    "        df_services, hierarchical.set_params(linkage=linkage)\n",
    "    )\n",
    "    \n",
    "r2_scores_services['birch'] = get_r2_scores(df_services,birch)\n",
    "r2_scores_services['spectral'] = get_r2_scores(df_services,spectral)\n",
    "\n",
    "pd.DataFrame(r2_scores_services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd556193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the R² scores for each cluster solution on services variables\n",
    "pd.DataFrame(r2_scores_services).plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Services Variables:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R² metric\", fontsize=13)\n",
    "plt.savefig('plot_services_r2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af15dd0",
   "metadata": {},
   "source": [
    "## Customer Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f207b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on customer variables\n",
    "r2_scores_customer = {}\n",
    "r2_scores_customer['kmeans'] = get_r2_scores(df_customer, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores_customer[linkage] = get_r2_scores(\n",
    "        df_customer, hierarchical.set_params(linkage=linkage)\n",
    "    )\n",
    "    \n",
    "r2_scores_customer['birch'] = get_r2_scores(df_customer,birch)\n",
    "r2_scores_customer['spectral'] = get_r2_scores(df_customer,spectral)\n",
    "\n",
    "pd.DataFrame(r2_scores_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the R² scores for each cluster solution on customer variables\n",
    "pd.DataFrame(r2_scores_customer).plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Customer Variables:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R² metric\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985babf3",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12.5.1\">\n",
    "\n",
    "### 12.5.1 K Means Clustering\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db4cd2",
   "metadata": {},
   "source": [
    "### Confirming optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034adacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_clusters = range(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a037fc",
   "metadata": {},
   "source": [
    "### Services Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for n_clus in r_clusters:  # iterate over desired ncluster range\n",
    "    kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=15, random_state=1)\n",
    "    kmclust.fit(df_services.copy())\n",
    "    inertia.append(kmclust.inertia_)  # save the inertia of the given cluster solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ddda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_inertia_plot(r_clusters,inertia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996a5d5",
   "metadata": {},
   "source": [
    "This plot shows us either 3 or 4 clusters, more inclined to choose 3 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2186c2",
   "metadata": {},
   "source": [
    "### Silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78504d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "\n",
    "# Storing average silhouette metric\n",
    "avg_silhouette = []\n",
    "for nclus in range(2,10):\n",
    "    # Skip nclus == 1\n",
    "    if nclus == 1:\n",
    "        continue\n",
    "    \n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "\n",
    "    # Initialize the KMeans object with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    kmclust = KMeans(n_clusters=nclus, init='k-means++', n_init=15, random_state=1)\n",
    "    cluster_labels = kmclust.fit_predict(df_services)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed clusters\n",
    "    silhouette_avg = silhouette_score(df_services, cluster_labels)\n",
    "    avg_silhouette.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {nclus}, the average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df_services, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(nclus):\n",
    "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        # Get y_upper to demarcate silhouette y range size\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        # Filling the silhouette\n",
    "        color = cm.nipy_spectral(float(i) / nclus)\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    plt.title(\"The silhouette plot for the various clusters.\")\n",
    "    plt.xlabel(\"The silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    # The silhouette coefficient can range from -1, 1\n",
    "    xmin, xmax = np.round(sample_silhouette_values.min() -0.1, 2), np.round(sample_silhouette_values.max() + 0.1, 2)\n",
    "    plt.xlim([xmin, xmax])\n",
    "    \n",
    "    # The (nclus+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    plt.ylim([0, len(df_services) + (nclus + 1) * 10])\n",
    "\n",
    "    plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "    plt.xticks(np.arange(xmin, xmax, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a18f42",
   "metadata": {},
   "source": [
    "### Average Silhouette Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8166e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_average_silhouette(range(2,10),avg_silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9e84e",
   "metadata": {},
   "source": [
    "For the k means method, 3 clusters is the overall optimal number of clusters for the services perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5eae9",
   "metadata": {},
   "source": [
    "### Customer Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for n_clus in range_clusters: \n",
    "    kmclust_cust = KMeans(n_clusters=n_clus, init='k-means++', n_init=15, random_state=1)\n",
    "    kmclust_cust.fit(df_customer.copy())\n",
    "    inertia.append(kmclust_cust.inertia_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a66919",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_inertia_plot(range_clusters,inertia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d967eee",
   "metadata": {},
   "source": [
    "### Silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b91b1ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "avg_silhouette = []\n",
    "for nclus in range(1,11):\n",
    "    if nclus == 1:\n",
    "        continue\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "    kmclust = KMeans(n_clusters=nclus, init='k-means++', n_init=15, random_state=1)\n",
    "    cluster_labels = kmclust.fit_predict(df_customer)\n",
    "    silhouette_avg = silhouette_score(df_customer, cluster_labels)\n",
    "    avg_silhouette.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {nclus}, the average silhouette_score is : {silhouette_avg}\")\n",
    "    sample_silhouette_values = silhouette_samples(df_customer, cluster_labels)\n",
    "    y_lower = 10\n",
    "    for i in range(nclus):\n",
    "    \n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        color = cm.nipy_spectral(float(i) / nclus)\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10  \n",
    "\n",
    "    plt.title(\"The silhouette plot for the various clusters.\")\n",
    "    plt.xlabel(\"The silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    xmin, xmax = np.round(sample_silhouette_values.min() -0.1, 2), np.round(sample_silhouette_values.max() + 0.1, 2)\n",
    "    plt.xlim([xmin, xmax])\n",
    "    plt.ylim([0, len(df_customer) + (nclus + 1) * 10])\n",
    "\n",
    "    plt.yticks([]) \n",
    "    plt.xticks(np.arange(xmin, xmax, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af9101",
   "metadata": {},
   "source": [
    "### Average Silhouette Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fbe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_average_silhouette(range(2,11),avg_silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8969c",
   "metadata": {},
   "source": [
    "Customer perspective with k means clustering has an optimal amount of 4 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb16bc",
   "metadata": {},
   "source": [
    "## Final K-Means Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d334891",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans_services = df_services.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmclust_services = KMeans(\n",
    "    n_clusters=3,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "km_labels_services = kmclust_services.fit_predict(df_kmeans_services)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans_customer = df_customer.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc9356",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmclust_customer = KMeans(\n",
    "    n_clusters=4,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "km_labels_customer = kmclust_customer.fit_predict(df_kmeans_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merging = df_before_models.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af09cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merging['services_labels'] = km_labels_services\n",
    "df_merging['customer_labels'] = km_labels_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f53fd",
   "metadata": {},
   "source": [
    "## Creating contingency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merging.groupby(['customer_labels', 'services_labels'])\\\n",
    "    .size()\\\n",
    "    .to_frame()\\\n",
    "    .reset_index()\\\n",
    "    .pivot('customer_labels', 'services_labels', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77224b12",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12.5.2\">\n",
    "\n",
    "### 12.5.2 Merging Perspectives\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b3077",
   "metadata": {},
   "source": [
    "### Manual Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge = [(0,1),(1,1),(2,1),(3,0)] #the 4 cluster we are going to merge\n",
    "\n",
    "df_centroids = df_merging.groupby(['customer_labels', 'services_labels'])\\\n",
    "    [metric_features].mean()\n",
    "\n",
    "euclidean = pairwise_distances(df_centroids)\n",
    "df_dists = pd.DataFrame(\n",
    "    euclidean, columns=df_centroids.index, index=df_centroids.index\n",
    ")\n",
    "\n",
    "source_target = {}\n",
    "for clus in to_merge:\n",
    "    if clus not in source_target.values():\n",
    "        source_target[clus] = df_dists.loc[clus].sort_values().index[1]\n",
    "\n",
    "source_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_merging.copy()\n",
    "\n",
    "for source, target in source_target.items():\n",
    "    mask = (df_['customer_labels']==source[0]) & (df_['services_labels']==source[1])\n",
    "    df_.loc[mask, 'customer_labels'] = target[0]\n",
    "    df_.loc[mask, 'services_labels'] = target[1]\n",
    "\n",
    "# New contigency table\n",
    "df_.groupby(['customer_labels', 'services_labels'])\\\n",
    "    .size()\\\n",
    "    .to_frame()\\\n",
    "    .reset_index()\\\n",
    "    .pivot('customer_labels', 'services_labels', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge = [(1,0),(2,2),(1,2)] #the 3 cluster we are going to merge\n",
    "\n",
    "df_second_merge = df_.copy()\n",
    "\n",
    "df_centroids = df_second_merge.groupby(['customer_labels', 'services_labels'])\\\n",
    "    [metric_features].mean()\n",
    "\n",
    "euclidean = pairwise_distances(df_centroids)\n",
    "df_dists = pd.DataFrame(\n",
    "    euclidean, columns=df_centroids.index, index=df_centroids.index\n",
    ")\n",
    "\n",
    "source_target = {}\n",
    "for clus in to_merge:\n",
    "    if clus not in source_target.values():\n",
    "        source_target[clus] = df_dists.loc[clus].sort_values().index[1]\n",
    "\n",
    "source_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, target in source_target.items():\n",
    "    mask = (df_second_merge['customer_labels']==source[0]) & (df_second_merge['services_labels']==source[1])\n",
    "    df_second_merge.loc[mask, 'customer_labels'] = target[0]\n",
    "    df_second_merge.loc[mask, 'services_labels'] = target[1]\n",
    "\n",
    "# New contigency table\n",
    "df_second_merge.groupby(['customer_labels', 'services_labels'])\\\n",
    "    .size()\\\n",
    "    .to_frame()\\\n",
    "    .reset_index()\\\n",
    "    .pivot('customer_labels', 'services_labels', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e906c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-name the cluster with the following order\n",
    "cluster_mapper_manual = {\n",
    " (0, 0): 1,\n",
    " (0, 2): 2,\n",
    " (1, 0): 3,\n",
    " (1, 1): 4,\n",
    " (1, 2): 5,\n",
    " (2, 0): 6,\n",
    " (2, 2): 7,\n",
    " (3, 1): 8,\n",
    " (3, 2): 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapped = df_.copy()\n",
    "df_mapped['merged_labels'] = df_mapped.apply(\n",
    "    lambda row: cluster_mapper_manual[\n",
    "        (row['customer_labels'], row['services_labels'])\n",
    "    ], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ad6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-name the cluster with the following order\n",
    "cluster_mapper_manual = {\n",
    " (0, 0): 1,\n",
    " (0, 2): 2,\n",
    " (1, 1): 3,\n",
    " (2, 0): 4,\n",
    " (3, 1): 5,\n",
    " (3, 2): 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f471397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second_mapped = df_second_merge.copy()\n",
    "df_second_mapped['merged_labels'] = df_second_mapped.apply(\n",
    "    lambda row: cluster_mapper_manual[\n",
    "        (row['customer_labels'], row['services_labels'])\n",
    "    ], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21defbef",
   "metadata": {},
   "source": [
    "### Merging using Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centroids = df_.groupby(['services_labels', 'customer_labels'])\\\n",
    "    [metric_features].mean()\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfdbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    affinity='euclidean', \n",
    "    distance_threshold=0, \n",
    "    n_clusters=None\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = 0.3\n",
    "dendrogram(linkage_matrix, truncate_mode='level', labels=df_centroids.index, p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'Euclidean Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a9087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-running the Hierarchical clustering based on the correct number of clusters\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    affinity='euclidean', \n",
    "    n_clusters=7\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)\n",
    "df_centroids['hclust_labels'] = hclust_labels\n",
    "\n",
    "df_centroids  # centroid's cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper between concatenated clusters and hierarchical clusters\n",
    "cluster_mapper = df_centroids['hclust_labels'].to_dict()\n",
    "\n",
    "df_second_merge = df_.copy()\n",
    "\n",
    "# Mapping the hierarchical clusters on the centroids to the observations\n",
    "df_second_merge['merged_labels'] = df_second_merge.apply(\n",
    "    lambda row: cluster_mapper[\n",
    "        (row['services_labels'], row['customer_labels'])\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "# Merged cluster centroids\n",
    "df_second_merge.groupby('merged_labels').mean()[metric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b292fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_second_merge.groupby('merged_labels')\\\n",
    "    .size()\\\n",
    "    .to_frame()\n",
    "\n",
    "df_counts = df_counts\\\n",
    "    .rename({v:k for k, v in cluster_mapper.items()})\\\n",
    "    .reset_index()\n",
    "\n",
    "df_counts['services_labels'] = df_counts['merged_labels'].apply(lambda x: x[0])\n",
    "df_counts['customer_labels'] = df_counts['merged_labels'].apply(lambda x: x[1])\n",
    "df_counts.pivot('services_labels', 'customer_labels', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ac780",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"13\">\n",
    "\n",
    "# 13. Cluster Visualization\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b8c39",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"13.1\">\n",
    "\n",
    "## 13.1 Using t-SNE\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7e923b",
   "metadata": {},
   "source": [
    "### Clustering solution using initial merge + hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85deeac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = df_second_merge.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea06b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim = TSNE(random_state=42).fit_transform(df_tsne[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=df_tsne['merged_labels'], colormap='tab10', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a1bd9",
   "metadata": {},
   "source": [
    "### Clustering Solution using only initial manual merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9651f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne_manual = df_mapped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim = TSNE(random_state=42).fit_transform(df_tsne_manual[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d847ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=df_tsne_manual['merged_labels'], colormap='tab10', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d2b9b",
   "metadata": {},
   "source": [
    "### Clustering Solution using second manual merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5919ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne_manual = df_second_mapped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim = TSNE(random_state=42).fit_transform(df_tsne_manual[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=df_tsne_manual['merged_labels'], colormap='tab10', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d02ded",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"13.2\">\n",
    "\n",
    "## 13.2 Using Polar Line Plot\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualization = df_second_mapped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d6f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://towardsdatascience.com/clustering-with-more-than-two-features-try-this-to-explain-your-findings-b053007d680a\n",
    "clusters = df_visualization[metric_features].copy()\n",
    "clusters['label']= df_visualization['merged_labels']\n",
    "polar=clusters.groupby(\"label\").mean().reset_index()\n",
    "polar=pd.melt(polar,id_vars=[\"label\"])\n",
    "fig4 = px.line_polar(polar, r=\"value\", theta=\"variable\", color=\"label\", line_close=True,height=500,width=700)\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba7588a",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"13.3\">\n",
    "\n",
    "## 13.3 Using Cluster Profiles\n",
    "    \n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a86c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_analysis = df_visualization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, compar_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    if compar_titles == None:\n",
    "        compar_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2, figsize=figsize, squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, label, color=sns.color_palette(), ax=ax[0])\n",
    "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1])\n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0].annotate(s=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].legend(handles, cluster_labels) # Adaptable to number of clusters\n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=-20)\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5731077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Profilling each cluster (product, behavior, merged)\n",
    "cluster_profiles(\n",
    "    df = df_cluster_analysis[metric_features + ['customer_labels', 'services_labels', 'merged_labels']], \n",
    "    label_columns = ['customer_labels', 'services_labels', 'merged_labels'], \n",
    "    figsize = (28, 13), \n",
    "    compar_titles = [\"Customer clustering\", \"Services clustering\", \"Merged clusters\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff51c7",
   "metadata": {},
   "source": [
    "# Feature Importance and outliers reclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb37e79",
   "metadata": {},
   "source": [
    "## R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_variables(df):\n",
    "    \"\"\"Get the SS for each variable\n",
    "    \"\"\"\n",
    "    ss_vars = df.var() * (df.count() - 1)\n",
    "    return ss_vars\n",
    "\n",
    "def r2_variables(df, labels):\n",
    "    \"\"\"Get the R² for each variable\n",
    "    \"\"\"\n",
    "    sst_vars = get_ss_variables(df)\n",
    "    ssw_vars = np.sum(df.groupby(labels).apply(get_ss_variables))\n",
    "    return 1 - ssw_vars/sst_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ea199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are essentially decomposing the R² into the R² for each variable\n",
    "r2_variables(df_cluster_analysis[metric_features + ['merged_labels']], 'merged_labels').drop('merged_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1c19d",
   "metadata": {},
   "source": [
    "PremiumMotor, PremiumLife and YearSal are the best variables to distinguish the different clusters beacuase they have the highest R square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d6024a",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X = df_cluster_analysis.drop(columns=['customer_labels','services_labels','merged_labels'])\n",
    "y = df_cluster_analysis.merged_labels\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fitting the decision tree\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"It is estimated that in average, we are able to predict {0:.2f}% of the customers correctly\".format(dt.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaef1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing feature importance\n",
    "pd.Series(dt.feature_importances_, index=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e065997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the cluster labels of the outliers\n",
    "df_outliers_dbscan['merged_labels'] = dt.predict(df_outliers_dbscan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers_dbscan['merged_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c620a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([df_outliers_dbscan,df_cluster_analysis]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_outliers = TSNE(random_state=42).fit_transform(df_outliers_dbscan[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9214462",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(two_dim_outliers).plot.scatter(x=0, y=1, c=df_outliers_dbscan['merged_labels'], colormap='tab10', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad826719",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_outliers = TSNE(random_state=42).fit_transform(all_data[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8255231",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(two_dim_outliers).plot.scatter(x=0, y=1, c=all_data['merged_labels'], colormap='tab10', figsize=(15,10))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
