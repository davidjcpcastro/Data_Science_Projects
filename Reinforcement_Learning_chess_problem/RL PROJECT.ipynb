{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Final Project \n",
    "\n",
    "Welcome to your Reinforcement Learning project focused on developing an RL agent capable of playing chess at a strategic level. Chess has long been considered a benchmark for measuring AI capabilities, and this project aims to leverage the power of RL to create an intelligent agent that can make optimal decisions in complex chess positions. By combining the principles of reinforcement learning with the rich strategic domain of chess, you will explore new approaches to create the most effective chess player.\n",
    "\n",
    "## Project Objectives:\n",
    "\n",
    "* Train an RL agent to play chess: The primary objective of this project is to develop an RL agent that can play chess at a high level of proficiency. The agent should be capable of evaluating chess positions and making strategic decisions.\n",
    "\n",
    "* Optimize decision-making using RL algorithms: Explore different RL algorithms, as seen in class, to train the agent. Compare and analise their effectiveness in learning and decision-making capabilities in the context of chess.\n",
    "\n",
    "* Use a challenging chess environment: Use a comprehensive environment for the agent to interact with, representing the rules and dynamics of chess. This environment will provide a realistic and challenging setting for the agent's training and evaluation.\n",
    "\n",
    "* Evaluate and benchmark performance: Assess the performance of the RL agent against different benchmarks from existing chess engines. You will compare your agent's performance to established chess engines to measure progress and identify areas for improvement.\n",
    "\n",
    "\n",
    "### Extra Objectives:\n",
    "\n",
    "* Investigate transfer learning and generalization: Explore techniques for transfer learning to leverage knowledge acquired in related domains or from pre-training on large chess datasets. Investigate the agent's ability to generalize its knowledge.\n",
    "\n",
    "* Enhance interpretability and analysis: Develop methods to analise the agent's decision-making process and provide insights into its strategic thinking. Investigate techniques to visualize the agent's evaluation of chess positions and understand its reasoning behind specific moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Play Chess! \n",
    "\n",
    "As you know [Chess](https://en.wikipedia.org/wiki/Chess) is a board game for two players, called White and Black, each controlling an army of chess pieces in their color, with the objective to checkmate the opponent's king.\n",
    "\n",
    "Chess is an abstract strategy game that involves no hidden information and no use of dice or cards. It is played on a chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. White moves first, followed by Black. Checkmating the opponent's king involves putting the king under immediate attack (in \"check\") whereby there is no way for it to escape.\n",
    "\n",
    "\n",
    "![](Images/CHESS_MOVES.PNG)\n",
    "\n",
    "* The king moves one square in any direction. There is also a special move called castling that involves moving the king and a rook. The king is the most valuable piece â€” attacks on the king must be immediately countered, and if this is impossible, the game is immediately lost.\n",
    "* A rook can move any number of squares along a rank or file, but cannot leap over other pieces. Along with the king, a rook is involved during the king's castling move.\n",
    "* A bishop can move any number of squares diagonally, but cannot leap over other pieces.\n",
    "* A queen combines the power of a rook and bishop and can move any number of squares along a rank, file, or diagonal, but cannot leap over other pieces.\n",
    "* A knight moves to any of the closest squares that are not on the same rank, file, or diagonal. (Thus the move forms an \"L\"-shape: two squares vertically and one square horizontally, or two squares horizontally and one square vertically.) The knight is the only piece that can leap over other pieces.\n",
    "* A pawn can move forward to the unoccupied square immediately in front of it on the same file, or on its first move it can advance two squares along the same file, provided both squares are unoccupied (black dots in the diagram). A pawn can capture an opponent's piece on a square diagonally in front of it by moving to that square (black crosses). It cannot capture a piece while advancing along the same file. A pawn has two special moves: the en passant capture and promotion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The [Environment](https://github.com/iamlucaswolf/gym-chess)\n",
    "\n",
    "The environment gym-chess provides OpenAI Gym environments for the game of Chess. It comes with an implementation of the board and move encoding used in AlphaZero. \n",
    "\n",
    "Please install it using the command: \n",
    "\n",
    "`pip install gym-chess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import gym\n",
    "import gym_chess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Two player's game\n",
    "\n",
    "As you know chess is played by two players, as such the gym-chess environment gives you access to both players actions in a sequential matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WHITE_PLAYER_POLICY(env, state):\n",
    "    legal_actions = env.legal_actions\n",
    "    action = np.random.choice(legal_actions)\n",
    "    #action = env.encode(chess.Move.from_uci(np.random.choice(legal_actions)))\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def BLACK_PLAYER_POLICY(env, state):\n",
    "    legal_actions = env.legal_actions\n",
    "    action = np.random.choice(legal_actions)\n",
    "    #action = env.encode(chess.Move.from_uci(np.random.choice(legal_actions)))\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"ChessAlphaZero-v0\"\n",
    ")  # We will use Alpha Zero's numenclature for the actions encodings\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "counter = 0  # Since each step represents a play in a chess game we are going to store the number of steps associated to the episode/game\n",
    "\n",
    "while not done:\n",
    "    if (\n",
    "        counter % 2 == 0\n",
    "    ):  # If the step number is pair, this means that it is the WHITE player's turn\n",
    "        action = WHITE_PLAYER_POLICY(env, state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "    else:  # If the step number is not pair, aka odd, this means that it is the BLACK player's turn\n",
    "        action = BLACK_PLAYER_POLICY(env, state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "    counter += 1\n",
    "    state = next_state\n",
    "print(reward)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The agent receives a reward of +1 when the white player makes a winning move, and a reward of -1 when the black player makes a winning move. \n",
    "\n",
    "All other rewards are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluationg your agent with [Stockfish](https://github.com/zhelyabuzhsky/stockfish)\n",
    "\n",
    "In order to have a good enough idea that our agent is actually playing well we need a benchmarkable opponent.\n",
    "\n",
    "As such we need to install stockfish a free and open-source chess engine. Stockfish has consistently ranked first or near the top of most chess-engine rating lists and, as of April 2023, is the strongest CPU chess engine in the world.\n",
    "\n",
    "`pip install stockfish`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stockfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stockfish import Stockfish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StockFish has a python api as seen above, nevertheless the engine still needs to be downloaded [here](https://stockfishchess.org/download/) and used in the path.\n",
    "\n",
    "NOTE: You were given an engine already in moodle, nevertheless different computer systems (Windows, Mac, Ubuntu) might require other Stockfish engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stockfish_path = \"C:/Users/nunoa/Desktop/Aulas/Reinforcement Learning/FINAL PROJECT/stockfish_15.1_win_x64_avx2/stockfish-windows-2022-x86-64-avx2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# David\n",
    "Stockfish_path = \"C:/Users/David/Desktop/Nova IMS/2Âº semestre/Reinforcement Learning/project/stockfish_15.1_win_x64_avx2/stockfish-windows-2022-x86-64-avx2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions bellow generate episodes/games for a WHITE or BLACK Pieces Scenario respectively. We store the outcome of the episode (win/draw/loss) and the number of steps taken.\n",
    "\n",
    "#### Notice how the AGENT_POLICY function is used it recieves as inputs the env and the current state.\n",
    "`action = AGENT_POLICY(env, state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_WHITE_scenario(Stockfish_path, AGENT_POLICY):\n",
    "    env = gym.make(\n",
    "        \"ChessAlphaZero-v0\"\n",
    "    )  # We will use Alpha Zero's numenclature for the actions encodings\n",
    "    stockfish = Stockfish(Stockfish_path)\n",
    "    stockfish.set_elo_rating(\n",
    "        100\n",
    "    )  # Default \"skill\" level is 1350, higher will increase the skill of stockfish \"player\". See more at https://en.wikipedia.org/wiki/Elo_rating_system\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    counter = 0  # Since each step represents a play in a chess game we are going to store the number of steps associated to the episode/game\n",
    "\n",
    "    while not done:\n",
    "        if (\n",
    "            counter % 2 == 0\n",
    "        ):  # If the step number is pair, this means that it is the WHITE player's turn\n",
    "            action = AGENT_POLICY(env, state)\n",
    "            decoded_action = str(env.decode(action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        else:  # If the step number is not pair, aka odd, this means that it is the BLACK player's turn\n",
    "            decoded_action = stockfish.get_best_move()\n",
    "            action = env.encode(chess.Move.from_uci(decoded_action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        counter += 1\n",
    "        state = next_state\n",
    "    env.close()\n",
    "\n",
    "    return reward, np.ceil(counter / 2)\n",
    "\n",
    "\n",
    "def generate_BLACK_scenario(Stockfish_path, AGENT_POLICY):\n",
    "    env = gym.make(\n",
    "        \"ChessAlphaZero-v0\"\n",
    "    )  # We will use Alpha Zero's numenclature for the actions encodings\n",
    "    stockfish = Stockfish(Stockfish_path)\n",
    "    stockfish.set_elo_rating(\n",
    "        100\n",
    "    )  # Default \"skill\" level is 1350, higher will increase the skill of stockfish \"player\". See more at https://en.wikipedia.org/wiki/Elo_rating_system\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    counter = 0  # Since each step represents a play in a chess game we are going to store the number of steps associated to the episode/game\n",
    "\n",
    "    while not done:\n",
    "        if (\n",
    "            counter % 2 == 1\n",
    "        ):  # If the step number is not pair, aka odd, this means that it is the BLACK player's turn\n",
    "            action = AGENT_POLICY(env, state)\n",
    "            decoded_action = str(env.decode(action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        else:  # If the step number is pair, this means that it is the WHITE player's turn\n",
    "            decoded_action = stockfish.get_best_move()\n",
    "            action = env.encode(chess.Move.from_uci(decoded_action))\n",
    "            stockfish.make_moves_from_current_position([decoded_action])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        counter += 1\n",
    "        state = next_state\n",
    "    env.close()\n",
    "\n",
    "    return reward, np.ceil(counter / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function bellow a visualization is produced from the bechmarks made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AGENT_EVALUATION(Stockfish_path, AGENT_POLICY, n_evaluations=100):\n",
    "    results_list = []\n",
    "\n",
    "    for evaluation_number in tqdm(range(n_evaluations)):\n",
    "        generate_episode = generate_WHITE_scenario\n",
    "\n",
    "        reward, n_steps = generate_episode(Stockfish_path, AGENT_POLICY)\n",
    "\n",
    "        if reward == 1:\n",
    "            result = \"VICTORY\"\n",
    "        elif reward == 0:\n",
    "            result = \"DRAW\"\n",
    "        else:\n",
    "            result = \"LOSS\"\n",
    "\n",
    "        results_list.append([\"WHITE\", result, n_steps])\n",
    "\n",
    "        generate_episode = generate_BLACK_scenario\n",
    "\n",
    "        reward, n_steps = generate_episode(Stockfish_path, AGENT_POLICY)\n",
    "\n",
    "        if reward == -1:\n",
    "            result = \"VICTORY\"\n",
    "        elif reward == 0:\n",
    "            result = \"DRAW\"\n",
    "        else:\n",
    "            result = \"LOSS\"\n",
    "\n",
    "        results_list.append([\"BLACK\", result, n_steps])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        results_list, columns=[\"AGENT COLOR\", \"OUTCOME\", \"N STEPS\"]\n",
    "    ).astype(\"int\", errors=\"ignore\")\n",
    "\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "    results_group = (\n",
    "        df.groupby([\"AGENT COLOR\", \"OUTCOME\"])\n",
    "        .count()\n",
    "        .rename(columns={\"N STEPS\": \"GAMES\"})\n",
    "    )\n",
    "\n",
    "    n_games = results_group.sum()[0]\n",
    "\n",
    "    results_group = (2 * 100 * results_group / (n_games)).astype(\"int\")\n",
    "\n",
    "    viz_df = (\n",
    "        results_group.reset_index()\n",
    "        .pivot_table(index=\"AGENT COLOR\", columns=\"OUTCOME\", values=\"GAMES\")\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    viz_df.plot(kind=\"barh\", stacked=True)\n",
    "\n",
    "    plt.xlabel(\"Percentage\")\n",
    "    plt.title(f\"EVALUATION RESULTS FOR {n_games} GAMES\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060473583fd04a20854ee5071faba839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqEAAAHrCAYAAAD/iUTJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABppklEQVR4nO3dd1hTZ8MG8DuAgCyjKAgCgrgXrjpwLxyouEBx1db2rXvUjfWttlbFWetqbd0iqHVPUHGAOOuuGxBEUHEgQ5nJ9wdfzguSQBLDAfT+XRfXpTnrOTkZd57zDElCQoIcREREREQi0ivqAhARERHR54chlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxD6GQoJCYFUKtXob9SoUQCA1NRUODg4QCqVombNmsjKytLo2PPmzRP2uWPHDqXr3LlzJ9exDxw4oNa+o6Ki8pRXHTmfjwULFqi1jZ+fn7CNn5+fWtusW7dO2KZ8+fJ48eJFvvvV9q9evXrC/hYsWCA8HhISUmAZ09PTERAQgK+++goNGzaEvb09bGxsUL9+fQwYMAAbNmxAUlJSgfvJeS2kUilat24NuTz/GYJznvuJEycKPEZB+/jwz9bWFnXr1oW3tzf8/f2Rnp5e4P4+5rn/UEJCAlavXo0+ffqgRo0asLKygp2dHRo0aAA3NzdMnjwZe/bswbNnz/Iti7u7u1rPRUHvB3Ve97p+PSrExcVh0aJF6N69O6pWrYoKFSrAwcEBjRs3Rrdu3eDj44PDhw/jzZs3ap2rMvXq1dPJdVO4d+8e5s2bh44dOwrXr2rVqmjVqhV8fHxw+fJltco1atQopWUoW7YsHBwc0LRpU4wcORJnzpzR+tw/FB8fj2PHjuGXX35B//79UaVKlXxfGwU5deoUvvzyS9SpUwdWVlaoWbMmPD09sW/fPp2VOaeHDx9iyZIl8PDwQL169VCpUiXh+W/fvj0mTJiA/fv34/379xrtVxffNYq/R48eqbWth4dHru3mzJmjdL3i9NlTmAxEPyKVaMbGxujbty82bdqEZ8+e4fTp0+jYsaNa28rlcuzcuRMAYG5ujp49eypdz9/fP8//e/Xq9XEFLwZynldmZiZ27dqFMWPGFGGJcjt+/DimTp2Kx48f51kWHR2N6OhoBAYGYsGCBfjpp5/g7e2t9r5v3bqFffv2oU+fPjossWbevXuHd+/eISYmBkePHsXKlSuxY8cO2NvbF/qxAwMDMXr0aLx69SrX4+np6UhOTsbjx49x6dIlrF+/HlZWVnjw4EGhl6mobN26FTNmzEBKSkquxzMyMpCYmIjw8HCcP38ea9asQZMmTbT+QaIrSUlJ8PHxgZ+fH2QyWa5lL1++xMuXL3H79m2sWbMG7u7uWLZsGaytrTU+jlwuR2JiIhITE/HgwQMEBASgd+/eWLduHQwNDT/qHKpVq/ZR2+cs45QpU7B+/fpcjz979gzPnj3D8ePH0b17d2zcuBFGRkYffbz4+Hj4+Phg9+7deZ574H/P/7Vr17B582ZYWFhg1KhRmDBhAkxMTArcvy6/awICAvDDDz/ku87Tp0/VqgzQpeL+2cMQ+pkbMWIERowYUeB6UqlU+Le3tzc2bdoEIPuNp24IPXfuHKKjowEAvXr1UvohkZWVhV27dgEAzMzMkJycjOPHj+Ply5coX768Wscpju7du4dr164B+N95+fv75wmh7u7uaNiwodJ9PHv2DH379gUAdO/eXeUHnjZfWBs3bsSUKVOEmu3OnTujd+/ecHZ2hoGBAaKjo3HkyBHs3bsX8fHxGDVqFMLDwwv80M1p4cKF8PDwgJ6eODdgfvjhB3Tv3l34/4sXL3Dnzh389ttvePbsGe7cuQNvb2+cOXMG+vr6+e6rYcOGWL16dYHHVPbcnz9/HkOGDEFGRgb09PTQt29fdOvWDU5OTtDT08Pr169x69YtnDp1CqGhoZqfaCHS9etxz549GDduHADAyMgIgwYNQvv27WFvbw+JRILnz5/jxo0bOHnypNo1iwWxsbHB7t27811H1XsmPj4e/fv3x40bNwAAVlZWGDx4MFq1agVLS0u8ffsWV65cgZ+fHyIiInD48GHcunULe/fuhbOzc4Fl27NnDypWrAgg+7MvJiYGFy9exO+//47U1FTs27cPFSpUwOLFizU8a9Xs7OxQvXp1BAcHa7ztL7/8IgTQ2rVrY+LEiahWrRqio6Px+++/4/z58zhy5AjGjx+PP/7446PKeevWLXh7eyMmJgZA9nPfp08ftGjRAhUrVoSxsTFevXqFR48e4dSpUwgODkZiYiJ8fX3RqVMnfPHFF/nuX1ffNcbGxkhNTcXOnTsxa9YsSCQSlevu3LkTMpkMpUuXVrvW9lP/7GEI/cyVL18etWvX1mibZs2awdnZGeHh4Th8+DCSk5NhZmZW4HYBAQHCv1XVogUHBwu3BBYuXIhx48YJtYba3DYqLhS/uE1NTfHf//4X06ZNw+3bt3Hr1q1ct1EUt1aUMTU1Ff5dpkwZja+bKqdOncL3338PuVwOMzMzbNiwAW5ubrnWadKkCfr27YuxY8fC29sbcXFxWLJkCRwcHDBs2LB8929paYlXr17h/v372LFjh0Y1qB/DxsYm13NUu3ZttGvXDkOGDIGbmxvu3buH27dv49ChQ/Dw8Mh3XyYmJlo/37NmzUJGRgb09fWxa9cudOjQIc86HTp0wIQJExAfH19otzS1ocvXY1ZWFnx8fABkf+kfOXIE9evXz7Nely5dMG3aNERFReHs2bPaF/7/GRgYaHXtZDIZhg8fLgTQHj16YNWqVXmej7Zt22Ls2LGYPXs21q1bh+joaHh7e+PUqVO5niNlnJ2dUblyZeH/9erVQ7du3eDp6YlOnTrh/fv3wg9EbWpXFaZNm4ZGjRqhUaNGsLKyQlRUFFxcXDTaR2RkJFasWCGU89ixY8L5NWzYEO7u7hg4cCBOnDiBHTt2YNiwYWjZsqVW5Y2Pj8eAAQMQGxsLABg/fjxmzJihtOKiY8eO+O677xAbG4sVK1bgr7/+UusYuvqu6datG/bt24fo6GiEhYXle86KJmjdu3cv8IeRwqf+2cM2oaSVgQMHAsi+xalOO5r3798L6zk4OKh8oyrCmp2dHQYPHozmzZvnerwkkslkQjMEd3d3eHt7o3Tp0gCK/rzevXuHkSNHQi6XQyKRYNu2bXkCaE4NGjTAvn37hC+DGTNmCF8Uqnh5eaFSpUoAAF9fX2RmZuruBLRgYWGBiRMnCv8/ffp0oR0rLi4OV69eBZAdYpR9CeRUoUIFfPvtt4VWnqJ05coV4Uv/q6++UhpAc6pcuTKGDh0qRtGUWrNmDc6dOwcAaNOmDTZt2qQykBsZGWHRokUYNGgQAODBgwf4+eeftT52nTp10K9fPwDZTXc+tpbKx8cHXbt2hZWVldb7WLNmDTIyMgAAixYtyhOwDQwMsGzZMuFOx2+//ab1sSZOnCh8rvj4+OCnn34q8Pa6ra0tfH19cfjwYVhaWhZ4DF1911SuXFnYNmdFy4euXr2Ke/fuAQAGDBig0TG0UVI+exhCSSsDBgwQbjvk98ZTOHz4MBITEwFkB1hltyzevn2LI0eOAAA8PT0hkUiEN+vNmzfx77//6qr4ojp16hTi4uIAZD9v5ubmQueSXbt2FWko8/Pzw/PnzwEAQ4cORbt27QrcpkaNGpg8eTKA7BD7+++/57u+kZGRsP7jx4+xbdu2jyu0DuSsBXr69GmhHUdxKxEAnJycCu04JUFJei4yMjKEW6CGhob47bffYGBQ8I3DBQsWCLdyN2/enKcdnibEeo2qQy6XC5/NVatWRYsWLZSu5+DggDZt2gDI/nGXnJys8bHu3LkjHKt+/fqYMmWKRts3b94cVapUyXcdXX/XKCpl9u/fj9TUVKXrKMJtgwYNUKNGDY32r42S8n5jCCWtODg4oFWrVgCA0NDQAj8kc/aEV3U7du/evcIb2MvLCwDQu3dvoYF7UdcaaktRbisrKyHkKT7w4uPji7TjRc5AOHr0aLW3GzFiBIyNjQFkB9mCer4PHTpUuO24ZMkSpKWlaVFa3cnZBlSdcKGtnO207t+/X2jHKQlK0nNx4sQJ4Yeju7s7HB0d1dquTJkyGDx4MIDsuz/q3nJVRqzXqDqioqKEz/iCbrG3bt0aAJCWlia0g9fE9u3bhc+T7777rlDakOv6u6Z3794wNjZGYmKiEG5zysjIwJ49ewD8L7AWtpLyfmMIJa0pwmTO283KvHjxQmgE37x5c5W/yhRv/Hr16qFWrVoAstukdenSBUB2raGmQ0IVtcTERBw+fBgA0K9fP+GLpUOHDsKtMXVqkgurbLdv3waQ3TatZs2aam8rlUqF2pBXr14V2KOyVKlSmDZtGoDsX+gbN27UstS6obgtBmT/oCosNWrUEML6sWPHSuwPKV3Ieft906ZNOHXqVBGWJn9hYWHCv3N2blNHzvXPnz+vdRnEeo2qI2eIKagWL2dPfG3CT86mB507d9Z4e3Xo+rumTJky6NatGwAoHXowMDAQr169goGBAfr37/+xxVdLSfnsYQj9zL18+RJ37twp8O/D4VSA7B7uinZBqsb8BLJ7BCre0KpqQSMiInDx4kUA//tlqqCoNXz+/LlWPTqL0r59+4RekDnbAenr6wttvo4ePYqEhATRy3b37l3hujRo0EDj7XPeLrx161aB6w8cOFD4glq+fDnevXun8TF1ISsrC2vXrhX+r86wUe/evVPrfRIfH59rO2NjYwwfPhxA9i3NUaNGoWnTppg9ezYOHDiAJ0+e6PTcirPKlSsLAS0tLQ19+vRBu3btMG/ePBw7dkzpuLm6kJmZWeB1+5Dixxmg+XujXr16QnMjdd4Xyjx//lz4YS+VStVqJlOYcrb7VrTvVsXOzk74tzbNCBS3wm1tbT+qDasqhfVdo6jhPHnyZJ7PAUVFQ6dOnTQe5eVT/+xh7/jP3Pr16/OM+abMwYMHhdssCmZmZujZsycCAgJw7949XL9+XekHtuINaGxsjN69eyvd//bt2wEAenp68PT0zLXMzc0N5cqVw+vXr+Hv719ov44Lg+LXZ40aNfI8NwMGDMDatWuRlpaGPXv24Ouvvxa1bDnbq2nzYZ9zG3Xavunr62PGjBkYMWIEnj9/jj///BMTJkzQ+LjaevHiBf7991/Mnz9fGP6nX79+aNasWYHbXrt2Da6urgWuN336dMycOTPXY3PmzMHjx49x7NgxANmdVnLWHNvY2KB169bw9PQsUa9tbaxatQoDBgwQnv/r16/j+vXrwvLKlSujXbt2GDhwoMp2h5qKi4sr8Np9+CPwY94bJiYmMDc3R2JiokZtQmUyGZ48eYKwsDDMmzdPKNN///tftUYfKUw523YW1OM/53JN24QmJiYKnZ8K6lz04sULvHz5UukyExMTlU0oCuu7pmPHjqhQoQLi4+Px999/Cz3s37x5g6CgIACqK2Hy86l/9rAmlD5KzjeVstvK//77r1Cr4O7ujjJlyuRZRy6XCzWpbdu2FcbNUyhVqpQwHuGRI0eKpNZQG5GRkcLtuA9/cQPZNSyKW+BFcatEky8WZXJ+MaozixIA9O3bF3Xq1AEArFixQuisVhjGjBmTa0aR6tWro0+fPrh8+TJMTU11MpahOoyNjeHv748tW7agVatWedq4xcXFYefOncIXgbLJAj4V5cqVEyYKaNSoUZ7lUVFR2Lx5M7p164YBAwbg9evXRVDK3O8NbQKgYpuC3hcuLi7C67NcuXJwcXHBqFGj8PTpU9jZ2WH16tWi/zhVJueYlqVKlcp33ZyD1KvqpKOKJs/7mjVr4OrqqvRP1SQghfldY2BgINzdynlncPfu3UhPT0eZMmXQtWtXtfenCyXhs4ch9DM3ffp0JCQkFPj3YS2oQps2bYTbL7t3787T0ztnuFL1KzAkJES4NaAsrAH/u02iGMC5JFCcu0QiyfOLW0FxXpcvX1Z72jddyfkhr6y5RUFyfmGYm5urtY1EIhHGinz9+jXWrFmj8XF1wcXFBf/5z3/U7vDRsmVLtd4nH9ZEKEgkEvTq1QuHDh1CREQEAgICMG3aNLi5ueV67i5fvoxu3boJIxZ8igwMDDB06FAEBwfj/v372Lx5MyZNmoS2bdsKbdiA7HZ0PXr00Oq1mZO9vX2B1+1DOd8b2vTwVmyj7vtCGTc3N9HaDxZEMaQcAKGmUpWcnQ5zXk91fOzzXpDC/q5R3JK/fv260KZX8T3Qp08frWaR+tQ/exhC6aNIJBLhjRcfH4+TJ08Ky7KysvD3338DACpWrIj27dsr3YfiTWpiYqJyKs8vvvhCmIFE17WGOYeLKqiXt7L1lA03lfMXd/PmzVV2LPD09BR+nYpdG5rzdpc27fFybqPOuHwK7u7uQi3YmjVrPmp+8Pz88MMPCAsLQ1hYGM6cOQM/Pz/0798fEokEYWFh6N69u8rbeYVJKpWia9eu8PHxwc6dO/Hw4UOsXLlSGIMyLi4Ov/zyi8rtdfUaLQ6sra3h4eGBH3/8Efv378fDhw/x008/CeHlzp07udrviuVj3hvv3r0TakALel/s2bNHeI2eOHECv//+uzDTz4YNGzBs2DC1r3dh0uQHa87lmtYiW1hYCDWtBTVlmDNnToE/Jj5U2N81DRo0EAaW37FjBx49eoR//vkHgHi94vPzsZ89hYEhlD5azhrOnLchTp8+LQxO7eXlpXRqxJSUFBw8eBBA9oe3nZ1drluoOf/Cw8MBABcvXkRERITOyp/zV766U6nl7FSj7Fb2uXPnEBUVBSC7h6yqc6pbt64wJ/KOHTtE/cKpVauWcE1ytstTl2ImGQC5Zn1Sx6xZswBktwH7mEGt86OYMal27dpwcXGBu7s7/vrrLyxZsgQAEB0dLUwhWZSMjY0xdOjQXG2zDxw4kGeubMXrVFev0eLI3Nwc48ePx4IFC4THiuLOR926dYV/a/reuHXrlvA+Luh94ezsLLxGmzRpgoEDByIwMFCoAQ0MDCyyuwU52draCv8uqLNRzvEpC+rEpIyiuU5sbKxOO6uJ9V2jqEndtWsX/Pz8AGSP06kY0L44UfezpzAxhNJHc3Z2RtOmTQFkt6NRtPPL2UZU1a/AAwcOaHXbRZe1hmXLlhX+re6tiJzrKZtFRZvyxcTE6GSaQnVZWFgIX7bh4eEaDaeSkJAgtHctX748qlevrtGxO3bsKHQ8WbduXZ6enYVpxIgRwqxQR48exZkzZ0Q7dn46duwoNG1JSEjI0x5S8TpV94u5oNdocTZ48GChqYQuf3CqK2dHEGXjPuYn5/rqdCj5kJ6eHpYtWyZM0+nr61todwvUlXNYpoI+Jx4+fKh0O3Upxp8GgOPHj2u8vSpifdd4eXlBT08PMTExQi2+GDMkfYyCPnsKE3vHk054e3vj0qVLQjuaPn36CONjuri4qJz7VvEGt7S0xKJFiwo8zooVK3Dz5k0EBATAx8dHJ7cZK1euDHNzcyQlJeWq3ctPztoRxS93hZxTmbq6umLEiBH57ksul2PcuHF4//49/P390bZtW81O4CMMHjxYOOe1a9fi119/VWu7jRs3Cp0OBg8erNV1+OGHH+Du7o6UlBQsW7YsV+1TYZszZw6OHz8OuVyOn3/+WdTnPD8VK1YUapI+7ERQp04dxMbG4unTp4iPj0eFChXy3VfO16iYz60uGBoaoly5cnjx4kWhDFZekI4dO6JixYp49uwZDh8+jKioqFxzvKuSmJgo9L4uXbq00FFFUxYWFpgyZQqmTp2KxMRErFixAnPmzNFqX7pQuXJl2NraIjY2VpjKVBXFOJ9GRkZo2LChxsfy9vbGqlWrAGT/QPX29tbJa0Cs7xobGxu0bdsWp06dEj4ji8Ot+ILk99lTmBhCSSf69OmDmTNnIjU1FTt27ICenp5wO1BVh6QnT54gJCQEQPbctup8YL948QI3b97EkydPEBoaqrLDlCb09fXRsmVLHDt2DPfv31c51JRCXFycUGNZt27dPOO+HTx4UGgT9tVXX6l1Xnv37sXhw4dx8OBBLFmyRLQhWQYPHowlS5bgxYsX2LJlC/r27StMu6fKw4cPsXjxYgDZbau+++47rY7dsmVLtGvXDqdPn8bGjRs1np7vY9SuXRs9evTAwYMHceXKFZw6dUplm+WPIZfL1f7yevfunVDLZGFhkauGHsjuzauoGdqxYwfGjh2rcl8ymQy7du0CkN3jV5saOV3T5Ll48uSJUDteFAO1GxoaYsyYMZg9ezbS09MxYcIE/P333wV2ZJs1a5ZQ7i+//BLlypXTugzDhg3DsmXLEBcXh7/++gsTJkzI85oQi0Qigbu7O/788088evQI58+fVzqEVnR0tPDZ2K5dO60+x+rUqYNu3brh6NGjuHHjBpYsWSJMdKEtsb9rBg0aJEx40LRpU7Vn3NIlXX72FCbejiedkEqlwowRYWFhwi/ZUqVKqezhmbMNpIeHh1rH6dWrl/DG0uUt+ZxBasKECSobuaempmLMmDFCD1FlAUxRLiMjI7WH5FCcf0pKilCLKgZTU1OsWbMGEokEMpkMgwcPznca0Rs3bsDDw0P4gbFw4cJc7cU09cMPPwDIfl4LmoNe13KGXnVqRrRx79499OnTR/gCVCUrKwtTpkwRfrx07949zxfIkCFDhC/1xYsX5xpQ/UMLFy4UBmHv27dvgbWmYjh+/DiGDx9eYBvL9+/fY8KECcJnQ48ePUQoXV6jR48Wgtbp06cxYsQIlUOKpaenY+bMmdi6dSuA7FmDZs+e/VHHNzIyEtosJycnF3nb0NGjRwudhqZPn56ng1JmZiYmT54sTIAxfvx4rY/166+/CsMnzZ8/Hz/++GOBbaHz65gk9neNp6cnnj9/jufPnwvtUMWmy8+ewsSa0M+cYsakghgaGqJq1ar5ruPt7Y29e/dCLpcLw1N07txZ5QwRijajZcuWLbD2TaFSpUpo0qQJLl++jAMHDmDx4sVKO11ERkYKjcLzU7NmTTRu3Bjt27fH4MGD4efnhxs3bqBFixYYMWIEvvjiC5QtWxbJycm4du0aNmzYIDRa79ChA4YMGZJrf0+fPhVqAtq3b6/2EC1du3aFkZER0tLS4O/vj0GDBqm1nS506tQJixYtwvTp05GUlIT+/fujS5cu6N27N5ydnaGvr48nT57gyJEj2L17t/AlM3nyZAwbNuyjjt2kSRN06dIFgYGBovdUd3FxEY59/vx5hIaG5mqPlpNi1hJ1VKtWTfiylsvlOHXqFE6dOgVHR0d069YNTZo0gZ2dHUxMTJCQkIAbN27Az88Pd+/eBZD9g07RcSsnqVSKxYsXY9SoUXj79i06d+6MIUOGCLeOMzIy8ODBAwQEBAhfPDY2Nmr1dr1165Za75fGjRtrNL1rTjKZDPv27cO+fftQu3ZtuLm5oVGjRrCxsYGRkRFev36NK1euYPPmzcIwOg4ODkXWeUxfXx+bNm1Cv379cPv2bezfvx8XLlzAkCFD0LJlS5QrVw5v377F1atXsW3bNuFzwcHBAQEBATrpDDZ8+HAsX74c8fHx+OOPPzB27FilYy0X5Pz587na1uZs86fss9LDwyNPLaaTkxMmTJiAJUuW4ObNm+jcuTMmTZqEqlWrIiYmBmvWrBHaiQ8YMKDAOebzY21tjR07dsDb2xuxsbFYsWIFAgIC0Lt3b7i6uqJixYowMTFBSkoKIiMjce7cOezfv1/Y3sTEJNf+CvO7prAVh8+ewsQQ+plTd8Yke3v7Aqeg69ixI6ytrXN1iFB1K/7SpUvCuJjdu3dXe7xGIPsX6uXLl5GcnIyDBw8qbW9z4cIFXLhwocB9jRw5Eo0bNwaQ/eu7dOnSWL9+PeLi4jBv3jyV2/Xu3RurV6/O84txx44dQs9CdX9xA9m3QNq1a4fAwECEhoYiOjpa1NuQ3377Lezt7TFt2jRER0cjMDAQgYGBStetUKEC5s6dq7OgPGvWLAQFBRXJUDRTp04VznPx4sUqQ6i6s5YA2bXFivaDJiYmkEqlSEhIwOPHjwscbqhGjRr466+/YG9vr3S5t7c3MjMzMW3aNLx//x5//vkn/vzzT6Xr1q5dG35+fmpNE3jkyBG1OuDMnz9f6xAqlUphamqKlJQUldNl5vTFF19gw4YNHzXW5seytrbGkSNHMGPGDAQEBOD58+dYunQpli5dqnT9bt26Yfny5XkGQdeWiYkJxowZgzlz5iAxMRHr1q3D1KlTNd7Pli1bVNbmKfusbNWqldJb6bNmzcKbN2+wfv163LlzB99++22edbp3766TES9cXFxw6tQpzJw5E/v27cPz58/xxx9/5DvBRJkyZfCf//wHEydOFB4r7O+awlZcPnsKC2/Hk87o6+vnGpS9bNmy6NKli9J1c34gahLWPlxfl7fkS5UqhSVLliA0NBQjR45EvXr1IJVKYWBgAAsLC9SsWRPDhw9HUFAQNm3apPRXsaI8hoaGQvMEdSmmNM05xqiYunbtisuXL2Pt2rXw8PCAo6MjTE1NUbp0adjZ2cHNzQ1Lly7F1atXdVpTW79+fY1fA7rSpEkToS3omTNncOnSJZ3u39HREY8ePcL+/fsxZcoUtG/fHvb29ihdujT09fWF15Wnpyc2bdqE0NDQAof1GTp0KG7evIlZs2bB1dUVFSpUQKlSpWBqagoHBwf06dMHGzZsQEhICJycnHR6Ph+jefPmePToEQICAjB27Fi0atUKtra2MDY2hoGBgTBk2ZAhQ7Br1y4EBQWJ/oWojIWFBdasWYPQ0FBMnjwZjRo1Ep7zcuXKoU6dOhg5ciSCgoLg7++vswCqMGLECKGN3tq1awtlEHd1SSQSLF26FHv37kWvXr1ga2sLQ0NDWFtbo1OnTti0aRO2b9+u1aDsylhbW2PDhg24cOECfHx80Lp1a6Emz9DQEBUqVICLiwuGDx+OjRs34v79+5g1a1auz+bi+F0jhsL47CkMkoSEhKIfCZeIiIiIPiusCSUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEulQamoqIiIikJqaWtRFoXzwOpUMvE4lA69TyVAcrxNDKJGOZWVlFXURSA28TiUDr1PJwOtUMhS368QQSkRERESiYwglIiIiItExhBIRERGR6BhCiYiIiEh0DKFEREREJDqGUCIiIiISHUMoEREREYmOIZSIiIiIRMcQSkRERESiYwglIiIiItExhBIRERGR6BhCiYiIiEh0DKFEREREJDqGUCIiIiISHUMoEREREYmOIZSIiIiIRMcQSkRERESiYwglIiIiItExhBIRERGR6BhCiYiIiEh0DKFEREREJDqGUCIiIiISHUMoEREREYmOIZSIiIiIRMcQSkRERESiYwglIiIiItExhBIRERGR6BhCiYiIiEh0DKFEREREJDqGUCIiIiISHUMoEREREYmOIZSIiIiIRMcQSkRERESiYwglIiIiItExhBIRERGR6BhCiYiIiEh0DKFEREREJDqGUCIiIiISHUMoEREREYmOIZSIiIiIRMcQSkRERESiYwglIiIiItExhBIRERGR6BhCiYiIiEh0BkVdANKc8/Y4vEqTFXUxSCUTAK+KuhBUIF6nkoHXqWTgdSoJLrcq6hLkxppQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERia5Yh9Dz589DKpViwIABSpdPnDgRUqkUrVu3Vrp80aJFkEqlWLFiBQCgXr16sLa2zveY1tbWqFevXq7H/Pz8IJVKsXz5cgDAqFGjIJVK1f7z8/MDALi7uxe47s2bNzV6joiIiIhKIoOiLkB+mjRpAlNTU5w/fx5ZWVnQ19fPtTw0NBQSiQS3b9/GmzdvULZs2TzLAagMqdpyd3eHg4NDrscOHz6M27dvw9vbO8+yD0Pt2LFjYWpqqnTfBYVkIiIiok9BsQ6hpUqVQrNmzRAcHIwbN26gUaNGwrJnz57h0aNH6NmzJw4ePIjQ0FD07NlTWJ6eno7Lly/DwsICLi4uOi1Xjx490KNHj1yPRUdH4/bt2xg0aFCBoXfcuHEMm0RERPRZK9a344H/1WIqajUVFP8fO3YszMzM8iy/cuUK3r9/jxYtWuSpQSUiIiKiolViQmhISEiux0NCQmBubo7GjRujWbNmSpfn3J6IiIiIio9ifTseABo2bAhzc3NcuHABmZmZMDDILnJoaCiaNWsGAwMDtGzZEj///DNevXoFS0tLYTmQN4RmZmZiwYIFKo+XmZlZSGfyPytXrlTaJtTY2BiTJk0q9OMTERHR5yk9Pb1Q929sbKz2usU+hOrr66NFixYICgrC9evX0aRJE8TFxSE8PBxDhgwBALRs2RJyuRyhoaHw8PBAeno6rly5AqlUmqdTUFZWFnx9fYviVASrVq1S+riFhQVDKBERERWa58+fF9q+9fX1UaVKFbXXL/YhFMiuzQwKCkJISAiaNGki1HK2atUKANCoUSOYmJgIIfTy5ct4//49OnbsCD293C0OjIyM8r0AYnQYun//PjsmERERkeisra1haGhY1MUAUIJCKJB9i33SpEkICQmBqakpGjZsCCC7F/0XX3whhFO2ByUiIiLKy9DQUKNb5oWp2HdMAoD69etDKpUK7UJztgdVaNWqFe7evYv4+PhCGx+UiIiIiHSjRIRQPT09uLq6IiUlBYcPH0ZERARatmyZax3F/4ODg3HlyhWUL18etWrVKoriEhEREVEBSkQIBf5Xq6noVKRoD6rQuHFjGBsbY8WKFUhNTUWrVq0gkUhELycRERERFaxEtAkF/hdC79y5AxMTk1yzJwHZHY5ydloqzrfiVQ3RBGRPCVq/fn2RS0REREQkrhITQuvUqQNLS0u8evUKTZs2RalSpfKs07JlyxIRQlUN0QQADg4ODKFERET0yZMkJCTIi7oQpBnn7XF4lSYr6mIQERFRCXK51TvY29uzdzwRERERfb4YQomIiIhIdAyhRERERCQ6hlAiIiIiEh1DKBERERGJjiGUiIiIiEQnSgh9+/Yt5s+fL8ahiIiIiKgEKNQQ+vbtWyxYsAD169fHkiVLCvNQRERERFSCaDxjUmJiIk6dOoWoqCiYmprCxcUFTZo0ybVOSkoKVq5cid9//x2JiYmQy+WoUKGCzgpNRERERCWbRiH04MGDGD9+PN6+fZvr8Q4dOmDr1q0oXbo0jhw5gkmTJiE+Pl4In+PGjcM333yj04ITERERUcmldgi9f/8+vvnmG6SnpwMALCwskJ6ejtTUVAQHB8PHxwdffPEFxo4dC7lcDisrK4wbNw4jRoxA6dKlC+0EiIiIiKjkUbtN6Lp165Ceno769esjLCwMUVFRiI2NRUBAAMqVK4eAgAD4+PhAIpFg4sSJuHbtGsaOHcsASkRERER5qB1Cz507B319ffz111+oVasWAEAikaBLly748ccfkZqaisTERCxatAg//vgjTExMCq3QRERERFSyqR1CY2JiUKlSJVSrVi3Psk6dOgEAypYti6+++kp3pSMiIiKiT5LaITQlJQW2trZKl9nY2AAAHB0doafH8e+JiIiIKH86TYyGhoa63B0RERERfaJYbUlEREREotNonNA7d+6gZ8+eWi2XSCQ4cOCAZqUjIiIiok+SRiE0MTERoaGhWi2XSCSalYyIiIiIPllqh9Dp06cXZjmIiIiI6DOidgidMWNGYZaDiIiIiD4j7JhERERERKJjCCUiIiIi0WnUMSmnK1euYMeOHbh48SJiY2ORmJgICwsL2NraolmzZvD09ETTpk11WVYiIiIi+kRIEhIS5Jps8PLlS4wePRonTpwAAMjleTdX9ITv0KED1qxZAysrKx0UlRSct8fhVZqsqItBREREJcjlVu9gb28PY2Pjoi4KAA1rQuPi4uDm5oanT59CLpejevXqaNOmDapUqQJTU1OkpKQgIiICISEhuH//PoKDg9GhQwcEBQWpnPKTiIiIiD4/GoXQoUOHIiYmBg4ODli+fDk6dOigct3g4GBMmjQJ0dHRGDZsmFBzSkRERESkdsek/fv3459//oGjoyOCgoLyDaAAhBpQBwcHXL16lbMlEREREZFA7RC6b98+SCQS+Pr6wtraWq1trK2t4evrC7lcjj179mhdSCIiIiL6tKgdQq9cuQKpVIrOnTtrdAA3NzdIpVL8888/GheOiIiIiD5NaofQly9fonLlyhrPAa+npwdHR0e8fPlS48IRERER0adJ7RBaqlQppKWlaXWQtLQ0lCpVSqttiYiIiOjTo3YItbGxQWRkJN69e6fRARTDNtnY2GhcOCIiIiL6NKkdQlu1aoW0tDRs2bJFowNs2bIFaWlpaNWqlcaFIyIiIqJPk9ohdMiQIZDL5Zg3bx4uXLig1jbnz5/HvHnzIJFIMGTIEK0LSURERESfFrVDaMOGDTF48GCkpKTAw8MD8+bNw4sXL5Su++LFC8ybNw+9e/fG+/fv4e3tjYYNG+qs0ERERERUsmk0d3xGRgYGDhyI4OBgSCQS6OnpoUaNGnmm7bx//z5kMhnkcjnat2+PHTt2sGOSDnHueCIiItJUcZs7XqMQCgByuRy//vorfvvtNyQkJPxvRxIJ5PL/7apMmTIYP348Jk6cCD09tStcSQ0MoURERKSpEh9CFVJSUnDixAmcP38eT58+RXJyMszMzGBra4sWLVqgc+fOMDU11XV5CQyhREREpLlPJoRqYtSoUUhOTsbWrVsL+1CfBYZQIiIi0tRnGUKdnZ3x5s0bvH79urAPRVSkUlNT8eTJk2L1Jqe8eJ1KBl6nkoHXqWQojteJjTWJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKIzUHdFf39/rQ+Snp6u9bZERERE9OlRO4SOHj0aEolEq4PI5XKttyUiIiKiT4/aIdTOzo5BkoiIiIh0Qu0QeuvWrcIsBxERERF9RtgxiYiIiIhExxBKRERERKJT+3a8MtevX8eFCxcQGxuLxMREWFhYwNbWFs2bN0eDBg10VEQiIiIi+tRoFUL37duHhQsX4sGDByrXqVatGmbMmIE+ffpoXTgiIiIi+jRpHEInTZqEzZs3Qy6XAwDMzc1RuXJlmJmZITk5GVFRUUhKSsKDBw8wYsQInDp1Cr/99pvOC05EREREJZdGIfTnn3/Gpk2bAAD9+vXDqFGj0KhRo1xDN8nlcly9ehVr167F7t27sW3bNlhaWuLHH3/UacGJiIiIqOSSJCQkyNVZ8eHDh2jRogX09PTwxx9/qHWbfffu3Rg5ciQAICwsDNWqVfu40hIVc6mpqXjy5Ans7e1hbGxc1MUhFXidSgZep5JB2XWSyWRITExERkZGEZeOFGQyGVJTU2FsbAw9Pc37pZcqVQoWFhZabauK2jWhmzZtQlZWFqZOnap2O89+/frhwYMHWLRoETZt2oRffvlF64ISERFR8Zeeno6EhASUKVMGZcqU4UQ3xYRMJkN6ejoMDQ01DpJyuRzp6el4+fIlpFIpDA0NdVImtUtx+vRplCpVSqjZVNeoUaNgYGCA06dPa1o2IiIiKmGSkpJgaWkJIyMjBtBPhEQigZGRESwtLZGUlKSz/aodQp8+fYqqVauiTJkyGh1AKpWiatWqiImJ0bhwREREVLLIZDLo6+sXdTGoEOjr60Mmk+lsf2qH0NTUVJQuXVqrg5QuXRppaWlabUtEREREnx61Q6ilpSWePHmi1UGePHkCS0tLrbYlIiIiok+P2iG0fv36ePnyJcLCwjQ6wLlz5/Dy5UvUq1dP48IRERER0adJ7RDao0cPyOVyzJgxAykpKWptk5ycjOnTp0MikaBnz55aF5KIiIiIPi1qh9CBAweiatWquH37Njw8PBAeHp7v+o8ePYKHhwf+/fdfODs7Y+DAgR9dWCIiIiL6NKgdQvX19bFlyxaYm5vj6tWraNasGQYMGIDVq1fj6NGjOHv2LI4ePYrVq1djwIABaN68Oa5evQpzc3Ns2bKFPeWIiIgoX2fPnsVXX32FOnXqwMrKCo6OjujatStWr16N1NTUPOvXq1cPUqk0333mXCckJARSqVTtP3d3d2E/mZmZ2LZtGzw9PVG9enVUqFABDg4OaN++PebNm4fo6Gilx4+IiMCUKVPQpEkT2Nraws7ODq6urpg9ezaePXumdJsFCxYIZZgzZ47Kc/Px8RHWW758ea5l7u7uuc6lXLlyqFixIsqVKyc8dvPmzXyfu8Km0bSdtWrVQmBgIL766ivcu3cPQUFBOH78eJ71FPPK16hRAxs3bkStWrV0U1oiIiL65GRmZmLKlCnYtGkTTE1N0alTJ1SpUgWJiYkIDg7GrFmzsHHjRuzcuRNVqlTR+jgODg6YPn16rseio6Ph7++PunXr5gqdivUV6wwaNAi3b9+GlZUV2rVrBzs7O6SkpODmzZtYvnw5Vq5cifPnz+cq39atW/H9998jMzMTbdq0Qbdu3SCTyXDlyhWsXLkSGzduxIYNG+Dm5qa0vAYGBggICMDs2bPzVOZlZGRg586dMDAwQGZmpspzHjt2LExNTSGXy5GVlQV9fX1h/FZra2v1n7xCoFEIBYCaNWsiNDQUe/bswd9//43z58/nGrjU3NwczZs3R//+/dGvXz/WgBIREVG+5s6di02bNqFRo0bYtm0bbG1thWVZWVnw9fXFokWL0L9/f5w+fRoWFhZaHady5cqYOXNmrsdCQkLg7++PevXq5VkGZA++369fPzx8+BDjx4+Hj49PnmlkIyIi4OPjg+TkZOGxwMBAjB8/HuXKlcP27dvRrFmzXNscOXIEI0aMwNChQxEUFAQXF5c8x+7UqROOHTuG48ePo2vXrrmWHT16FC9fvkS3bt1w9OhRlec8btw4WFtbf9SMSYVFq1Lo6+vD09MTO3bsQHR0NKKjo3Hnzh3h3zt37oSXlxcDKBEREeUrPDwcq1evRtmyZREQEJArgALZmcPHxweenp6IiIjAypUrRS3fypUr8fDhQ3h5eeGnn37KE0ABoEqVKggICEDNmjUBZAfnadOmQS6XY/369XkCKAB0794dCxcuRFpaGmbMmKH02D179kSZMmWwbdu2PMv8/PxQvnz5POG0JNFJFDY3N4eNjQ3Mzc11sTsiIiL6TGzfvh0ymQzDhw+HlZWVyvWmTp0KIDt8iUlxvA9v4yujmFP97NmziIqKwhdffIF27dqpXH/IkCGwsbHB+fPnERERkWe5sbEx+vXrh8DAQLx8+VJ4PC4uDidOnICXlxcMDDS+qV1saFTyu3fvIjIyEuXLl0fTpk0LXP/ixYt49eoVqlSpIvw6ICIiIlK4ePEiAKBt27b5rle9enXY2NggNjYWMTExsLOzK/SyRUdH4+nTp6hUqRKcnZ3V3k7dc9LX10erVq2wa9cuXLp0SWl71yFDhmDDhg3YsWMHxowZAwDw9/dHVlYWhgwZgmvXruV7jJUrVyptE2psbIxJkyapfU6FQe0QmpaWBk9PTzx79gz79u1Ta5uMjAwMGzYMlSpVwpUrV1CqVClty0lERESfoBcvXgAAKlWqVOC6lSpVQlxcHJ4/fy5KCFWU7cMmAupup+45AcDz58+VLm/UqBFq164NPz8/IYRu375deLygELpq1Sqlj1tYWBR5CFX7dvyhQ4fw9OlT9O/fH61atVJrm1atWsHT0xNPnjzBgQMHtC4kERERkWL0HUXv7k+BOuc0ePBg3LlzB1evXkVYWBgePXqEIUOGqLX/+/fvIyEhAa9fv8azZ8/w+vVrJCQkqBxSSkwahVCJRIKRI0dqdIDRo0dDLpfj4MGDGheOiIiIPm2KdqBPnz4tcN3Y2Nhc2yh6ectkMpXbyOVyrUOr4jhxcXFabafNOSkzYMAAlCpVCtu2bcO2bdtgbGyMvn37alSm4kjtEHrt2jWUK1cODRo00OgA9erVg6WlJa5fv65h0YiIiOhTp+g5fubMmXzXe/DgAeLi4oQB3wEIQzW9fv1a6TZyuRxv3rzRekgnBwcH2NraIiYmpsCZInNS95yysrJw7tw5AMi3r0358uXRpUsX/P3339i/fz969uxZ4CD9JYHaITQ+Pl6ttg3KVKpUSWgfQURERKTg7e0NPT09bN68OVcP8A8tWbIEQPataYXatWsDAC5duqR0m9u3byMlJQV16tTRunyK296LFy8ucN309HQAQOvWrWFvb4/Lly/nG0T9/PwQGxuLFi1aFDgI/5AhQ5CYmIiUlBS1b8UXd2qH0PyqutWhaPNAREREpFC1alWMHDkSr1+/xsCBA/NMZSmTybBo0SLs3LkTTk5OGDdunLBs0KBBAID58+cjISEh13ZpaWn48ccfAQADBw7Uunzjxo1DtWrVEBAQgJ9++glpaWl51nn8+DEGDRqEe/fuAcie6WjhwoUAgBEjRuDKlSt5tgkMDMTMmTNhZGSEBQsWFFiOzp07w8/PD35+fmjTpo3W51OcqN073tLSUq22Dco8ffoUlpaWWm1LREREn7affvoJiYmJ2LZtGxo3bgw3Nzc4OTkhKSkJwcHBCA8Ph7OzM3bt2pXr1nrbtm0xcuRI/P7772jSpAm6desGa2trvH79GkFBQYiJiUGPHj0+qubQ3Nwcu3fvxqBBg7Bs2TL4+fmhffv2qFSpEt69e4ebN2/i4sWLMDAwwLx584Tt3N3d8euvv2LKlClwc3NDmzZtUL9+fWHazgsXLsDMzAxbtmxRq6mjvr5+nmlF1aFqiCZFGevXr6/xPnVF7RBat25dBAUF4fr16xq1C7127RpevXqlcl5UIiIi+rwZGBhg1apV6N+/PzZt2oQLFy7g0KFDMDExQY0aNfDVV19hxIgRKF26dJ5tFy5cCFdXV2zevBlHjhzB27dvYWpqijp16mDatGkYMmTIR09T6eDggFOnTmHHjh3Yt28fgoOD8ebNGxgbG6NKlSoYP348vv766zzDRg0fPhytWrXC2rVrcfr0aVy8eBESiQQODg4YO3YsxowZAxsbm48qW0FUDdGkOK+iDKGShIQEte6Tr1+/HlOmTEHnzp2xc+dOtQ/g6emJkydPYsmSJfj666+1LihRSZCamoonT57A3t5e6dRuVDzwOpUMvE4lw4fXKT4+HhUqVCjqYtEHdDV3vC6vr9qlGDx4MKysrHDixAlMmTIFmZmZ+a6fmZmJKVOm4MSJE7CyshLabRARERERqR1CjY2N8fvvv0NPTw8bNmxA8+bNsXbtWty6dQtJSUmQy+VISkrCrVu3sGbNGrRo0QIbNmyAvr4+Vq9ezV+xRERERCTQaO749u3bY/369Rg7dizCw8Mxa9YslevK5XKYmppi1apV6Nix40cXlIiIiIg+HRo3CvDw8MC5c+cwfPhwmJmZQS6X5/kzMzPD8OHDce7cOfTu3bsQik1EREREJZlGNaEKDg4OWL58OZYuXYrbt28jNjYWSUlJMDMzg62tLerVq/fRPdGIiIiI6NOlVQhV0NPTQ/369Yu0ez8RERERlTysriQiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhEp/Y4oeXKlUPz5s1x5MiRwiwPERERfcKkG58WdRFUSviqUlEX4bOidk2oYkpOIiIiIlIuKioKUqkU/fr1U2v9s2fP4quvvkKdOnVgZWUFR0dHdO3aFatXr0ZqaqrK7QIDA+Hl5YWqVauifPnyqFKlClq0aIExY8bg8OHDedaPiYnB9OnT0aRJE1hbW6NSpUpwcXGBl5cXfv31V6SkpGh9ztr6qBmTiIiIiEhzmZmZmDJlCjZt2gRTU1N06tQJVapUQWJiIoKDgzFr1ixs3LgRO3fuRJUqVXJtu3DhQixcuBAmJibo0qULHBwckJiYiMjISOzduxfh4eFwd3cX1r916xZ69OiBt2/folmzZujUqRMMDQ0RFRWFa9euISgoCL169cpznMLGEEpEREQksrlz52LTpk1o1KgRtm3bBltbW2FZVlYWfH19sWjRIvTv3x+nT5+GhYUFgOya1kWLFsHOzg7Hjx+HjY1Nrv2+f/8eV65cyfXYrFmz8PbtW6xcuRKDBw+Gnl7uG+GXLl1CuXLlCulMVWPHJCIiIiIRhYeHY/Xq1ShbtiwCAgJyBVAA0NfXh4+PDzw9PREREYGVK1cKy65evQqZTIYePXrkCaAAULp0abRu3TrXY5cvX0aZMmXg6emptDxNmzaFVCr9+BPTEEMoERERkYi2b98OmUyG4cOHw8rKSuV6U6dOBQD4+fkJj5UtWxYAEBkZqfbxypYti5SUFLx48ULLEhcOjW7HX7x4UevqWolEglevXmm1LREREdGn4uLFiwCAtm3b5rte9erVYWNjg9jYWMTExMDOzg5NmjRBpUqVEBgYiEGDBqFfv35o1KgRHB0dIZFIlO7Hw8MDa9euRa9evTBixAi0bNkStWvXhrGxsc7PTRMa14Qqeslr80dERET0uVPUSFaqVPCQUIp1nj9/DgAwMzODn58fatasiSNHjmDEiBFo2LAhKleujAEDBuDgwYN59vHf//4XAwcORHR0NGbPno0OHTqgUqVKaNu2LRYvXoyEhATdnZwGNKoJdXZ2xqRJkwqrLERERESUg6ISL2ctZ4MGDXD+/HlcunQJISEhuH79Oi5cuIDAwEBh6KY//vhD2KZ06dJYs2YNpk2bhtOnT+PatWv4559/cOPGDdy4cQObNm3C4cOH4ejoKOq5aRRCy5cvj0GDBhVWWYiIiIg+eVZWVnjw4AGePn2KatWq5btubGyssE1OEokEzZo1Q7NmzQBkh9XDhw9j1KhR2LlzJ3r27ImePXvm2sbW1hbDhw/H119/DSC7XemYMWMQFhaGmTNnwt/fX1enqBZ2TCIiIiISkSI4njlzJt/1Hjx4gLi4ONja2sLOzi7fdSUSCXr06IFRo0YByB4EvyBOTk5Ys2YNACAkJESdousUQygRERGRiLy9vaGnp4fNmzfj5cuXKtdbsmQJAGDw4MFq79vU1FSjsmi6vi4xhBIRERGJqGrVqhg5ciRev36NgQMH4tmzZ7mWy2QyLFq0CDt37oSTkxPGjRsnLPvnn3/g7++vdErP+Ph4bNmyBQDQokUL4XFfX1/ExMTkWV8ul2PZsmUAgObNm+vk3DTBGZOIiIiIdOzOnTvCrfEPubi44KeffkJiYiK2bduGxo0bw83NDU5OTkhKSkJwcDDCw8Ph7OyMXbt2CbMlAUBcXBxGjRqFadOmwdXVFdWqVYOBgQGio6MRGBiIlJQUdOnSBb179xa2Wb16NRYuXAgXFxc0bNgQ5cqVw+vXr3H27FmEh4ejXLlymDdvXmE/JXlIEhIS1Bo7afv27bCyskKnTp0Ku0xEJVZqaiqePHkCe3v7Ih9/jVTjdSoZeJ1Khg+vU3x8PCpUqFDUxSoyUVFRcHFxyXed7t27Y/v27QCA06dPY9OmTbh48SJevnwJExMT1KhRQxjTs3Tp0rm2TUpKwtGjR3Hy5EncvHkTsbGxSElJgVQqRZ06ddC/f38MGjQI+vr6wjZhYWEICgpCaGgonj59ipcvX8LIyAiVK1dGhw4dMGbMGFSsWFGt89Pl9VU7hBJRwfilWTLwOpUMvE4lA0NoySCTyZCeng5DQ8M8c8drQpfXV+3b8brotu/t7f3R+yAiIiKikk/tEDp69GiV00GpiyGUiIiIiAANQmjNmjW1CqEPHjxAVlbWRwdYIiIiIvp0qB1Cz58/r9GOQ0NDMXfuXMhkMgDZsy0REREREQGFME7o7du34eXlhV69euHKlSswNTXFtGnTcPXqVV0fioiIiIhKKJ2NE/r48WPMnz8fu3fvhkwmg6GhIb788ktMmzaNtaBERERElMtHh9CXL1/C19cXW7ZsQUZGBgDA09MTs2bNQuXKlT+6gERERET06dE6hCYnJ+O3337D2rVrkZKSArlcjs6dO+O///0v6tatq8syEhERUQkil8vZIfkTJJfrdmh5jUNoRkYG/vzzTyxbtgyvX7+GXC5H06ZN8eOPP8LV1VWnhSMiIqKSxdjYGKmpqXlm+qGSLzU1VacTR2gUQv39/bFgwQLExMRALpejZs2a+OGHH+Du7q6zAhEREVHJZWpqilevXgHIDqSsES355HI5UlNTkZycDEtLS53tV+0Q6urqinv37kEul8POzg4zZ86Et7c3X1xEREQk0NPTg6WlJVJSUvDy5cuiLg79P5lMJtRkajNtp7GxMSwtLT9qys8PqR1C7969C4lEAgMDA1SrVg0HDhzAgQMH1D6QRCLRydSfREREVLzp6enB3Nwc5ubmRV0U+n+pqalITEyEtbW1Tm+pfwyNbsfL5XJkZmbi1KlTGh+INaZEREREpKB2CJ0+fXphloOIiIiIPiNqh9AZM2YUZjmIiIiI6DOi82k7iYiIiIgKwhBKRERERKJTO4SOGTMGy5cvV7rs3LlzuHXrVr7btm3bVvPSEREREdEnSe0Qun37dgQFBSld1qNHj3w7LkVEROQbUomIiIjo86Kz2/G6nk+UiIiIiD5dbBNKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOrWn7QSA9PR0PHnyRONlaWlpmpeMiIiIiD5ZGoXQa9euwcXFJc/jEolE5TIiIiIiog9pFEI/ZixQiUSi9baUm/P2OLxKkxV1MUglEwCviroQVCBep5KB16lk4HUqCS63KuoS5KZ2CL1x40ZhloOIiIiIPiNqh1AHB4fCLAcRERERfUbYO56IiIiIRMcQSkRERESi06hjUnh4OKKiolCuXDk0aNAg17KBAweq3O6bb75Bp06dtCogEREREX161A6hMpkMXl5eiIyMxPbt2/MsDwwMhEQiUdqD/tGjR7h8+TJ7yBMRERERAA1CaHBwMCIiItCpUyd07dpV6TqVKlXC4MGDcz12+vRpXLp0CadOnUKHDh0+rrRERERE9ElQO4QeOXIEEokE3333ncp17OzsMGPGjFyPtWzZEr169cKhQ4cYQomIiIgIgAYdk65evQojIyO0bt1aowO0bt0aUqkUV69e1bhwRERERPRpUjuERkVFwd7eHkZGRhofxN7eXuW88kRERET0+VH7dnxycjKqVq2qcnlAQADKli2rdJmxsTGSkpI0Lx0RERERfZLUDqGmpqZITExUubxLly4ql719+xYmJiaalYyIiIiIPllq3463trZGVFQU0tLSNDpAamoqHj9+DGtra40LR0RERESfJrVDaNOmTZGeno5jx45pdICjR48iPT0dTZs21bhwRERERPRpUjuE9u7dG3K5HPPnz0dycrJa2yQlJWH+/PmQSCTw8PDQupBERERE9GlRO4R27NgRjRs3xsOHD9GvXz9ER0fnu/7jx4/Rr18/hIeHo1GjRpy2k4iIiIgEkoSEhLzzbKoQFRWFDh064M2bNzAwMICbmxtatWqFypUrw9TUFCkpKYiKikJoaCiCgoKQkZGBcuXKITg4GJUrVy7M8/isOG+Pw6s0WVEXg4iIiEqQy63ewd7eHsbGxkVdFAAahlAAiIyMxJAhQ3Dnzh2Vc8Er5o+vVasWtm7dCmdn548vKQkYQomIiEhTxS2Eqn07XsHJyQkhISH4888/0blzZ5iZmUEulwt/ZmZmcHNzw7p16xAaGsoASkRERER5aFwTqkxycjKSkpJgbm4OMzMzXZSL8sGaUCIiItJUia8JVcbMzAw2NjYqA+itW7cwffp0XRyKiIiIiD4BOgmhyiQkJGDdunVo27Yt2rZtiz///LOwDkVEREREJYza03aqQy6XIzg4GH5+fjhy5AjS09OFTkp169bV5aGIiIiIqATTSQiNjIyEn58fAgICEBsbCyA7kJYrVw79+/fH4MGDUb9+fV0cioiIiIg+AVqH0Pfv32Pfvn3Ytm0bzp8/DyA7eBoYGCAzMxPly5fHnTt3UKpUKZ0VloiIiIg+DRqH0EuXLmHbtm3Yt28fkpOThdvttWvXhre3N7y8vFCjRg3o6+szgBIRERGRUmqH0BUrVsDPzw+PHj0SgqelpSX69euHQYMGwcXFpdAKSURERESfFrVD6Jw5cyCRSITpOgcOHIiuXbvCwECnfZuIiIiI6DOg8RBNRkZGKFOmDMqUKcMASkRERERaUTuEfv/997C1tUVycjL8/f3h4eGB+vXr45dffkF4eHhhlpGIiIiIPjEaTdupGAd069atOHbsGNLS0iCRSAAATZo0waBBg9C7d284OTnB2toa9+7dK7SCf844bScRERFpqrhN26n13PEJCQnYsWMH/Pz8cOvWreydSSQwNDREWloaypcvjwcPHgghlXSHIZSIiIg0VdxCqNbTdkqlUnz33Xc4e/Yszp49i2+//RZly5ZFWloaAODVq1eoWbMm/vvf/+Lu3bs6KzARERERlXxa14Qqk5GRgcOHD8PPzw/BwcGQyWRCTWjDhg1x8uRJXR3qs8aaUCIiItJUcasJ1WkIzSkuLg5+fn7w9/dHREQEJBIJXr9+XRiH+uwwhBIREZGmilsI1fp2fEFsbGwwZcoU/PPPPzh06BC8vb0L61BEREREVMKIMtBny5Yt0bJlSzEORUREREQlQKHVhBIRERERqcIQSkRERESiYwglIiIiItExhBIRERGR6Ip9CI2KioJUKs3zZ2trC1dXVyxcuBDJycm5tqlXrx6sra01Ptbjx49RtmxZSKVSrFu3rsD1ExISsHjxYnTu3BlOTk4oX748nJ2d4eHhgT/++CNXuRTn0a9fP6X7WrFiBaRSKVxcXBAZGalx2YmIiIhKElF6x+uCk5MTvLy8AGTPYf/q1SscP34cCxcuRHBwMI4ePQp9ff2POsa2bdsgl8shkUiwdetW/Oc//1G57pkzZzB8+HC8efMGNWrUQO/evVGuXDm8fv0aYWFhmD59OtauXYvr168XeNy5c+di+fLlqFWrFvbs2QMbG5uPOg8iIiKi4q7EhNAqVapg5syZuR5LS0tD586dcenSJZw7dw5t2rTRev9ZWVnYvn07KlasiDZt2mDnzp24fv06GjRokGfdW7duYeDAgQCAdevWCeE4p5CQEPz000/5HlMmk2Hy5MnYuHEjmjRpgl27dqFs2bJanwMRERFRSVHsb8fnx8jICK1btwaQPVf9xzh58iRiY2PRv39/DB48GEB2zagy06dPx/v37+Hr66s0gAJA69atcejQIZXHy8jIwDfffIONGzeiXbt22LdvHwMoERERfTZKdAhNT09HaGgoJBIJ6tWr91H72rp1KwBg4MCBaN26Nezs7LBr1y6kpqbmWi8iIgJhYWGoVKkShgwZku8+jYyMlD7+7t07DBo0CHv27EHPnj2xc+dOmJmZfVT5iYiIiEqSEnM7PiIiAgsWLACQ3Sb09evXOHnyJOLi4vDTTz+hatWqWu/75cuXOHbsGGrXro26desCALy8vLBs2TIcOHAgV23nhQsXAGTPAqWnp3mGT0xMRN++fXHhwgUMGTIEK1as+Oi2rERERETqSE9PL9T9azIvfYkJoZGRkfD19c3zeLdu3eDm5vZR+/b390dGRkau+e0HDhyIZcuWYevWrblC6IsXLwAAlSpV0upYly9fBgA0bdoUq1at+ohSExEREWnm+fPnhbZvfX19VKlSRe31S0wI7dixI3bv3i38Pz4+HmfOnMH06dPh5uaGkydPal0b6ufnBz09PfTv3194rHr16mjUqBFCQ0Px+PFjODo6fuwpAABq1qyJt2/f4tKlS/D19cX06dN1sl8iIiKiglhbW8PQ0LCoiwGgBLcJrVChAvr37485c+bg7du3WL58uVb7uXz5Mu7du4e2bdvmGRrJ29sbcrk8VwclKysrAEBsbKxWx6tUqRIOHz6MSpUqYcGCBUITAyIiIqLCZmhoCGNj40L700SJDaEKjRs3BgDcuHFDq+0VHZJOnTqVZ0D8qVOnAsi+XS+TyQAAzZs3BwCcO3dOeExTVapUwaFDh2BnZwdfX1/Mnz9fq/0QERERlVQl5na8Km/evAEArQJhSkoK9u7dCxMTE5UzGV25cgV3797FyZMn0blzZ1SpUgWurq4ICwvD9u3b8+0hn5aWprKHvJOTEw4dOoSePXti0aJFkMlk+OGHHzQ+ByIiIqKSqESHUJlMJkyv6erqqvH2e/fuRVJSEry9vbFy5Uql6xw5cgSDBg3C1q1b0blzZwCAr68v3NzcMG3aNJiYmKBv3755tgsLC8PcuXMRGBio8viOjo5CEF2yZAnkcjlmz56t8XkQERERlTQlJoTmHKIJyB6cPiQkBPfv34ednR2mTJmSa/2MjAyMGjVK6b5MTEywdOlSoa1nfrWZbm5usLKywtGjR/Hy5UuUL18e9erVQ0BAAIYPH46vv/4aixYtgqurK8qWLYs3b97gwoULuHPnjlo9xCpXrozDhw+jZ8+eWLp0KWQyGX788Ud1nhIiIiKiEqvEhNAPh2gyMjKCg4MDxowZg++//x6Wlpa51pfJZPD391e6LwsLC4wcORIXLlyAk5MTWrZsqfK4BgYG8PLywqpVqxAQEICxY8cCANq2bYurV6/ir7/+QlBQEPbs2YPk5GRYWFigdu3aWLhwIYYOHarWudnb2ws1osuXL4dMJsPcuXPV2paIiIioJJIkJCTIi7oQpBnn7XF4laZdpygiIiL6PF1u9Q729vYa92IvLCW+dzwRERERlTwMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOoZQIiIiIhIdQygRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoDIq6AKS58EE2RV0EUiE1NRVPnjyBvb09jI2Ni7o4pAKvU8nA61Qy8DqVDNnX6V1RFyMX1oQSERERkegYQomIiIhIdAyhRERERCQ6hlAiIiIiEh1DKBERERGJjiGUiIiIiETHEEpEREREomMIJSIiIiLRMYQSERERkegYQomIiIhIdAyhRERERCQ6hlAiIiIiEh1DKBERERGJjiGUiIiIiETHEEpEREREomMIJSIiIiLRMYQSERERkegYQomIiIhIdAyhRERERCQ6hlAiIiIiEh1DKBERERGJjiGUiIiIiETHEEpEREREomMIJSIiIiLRMYQSERERkegYQomIiIhIdAyhRERERCQ6hlAiIiIiEh1DKBERERGJjiGUiIiIiETHEEpEREREomMIJSIiIiLRMYQSERERkegYQomIiIhIdAyhRERERCQ6hlAiIiIiEh1DKBERERGJjiGUiIiIiETHEEpEREREomMIJSIiIiLRMYQSERERkegYQomIiIhIdAyhRERERCQ6hlAiIiIiEh1DKBERERGJjiGUSMf09fWLugikBl6nkoHXqWTgdSoZitt1kiQkJMiLuhBERERE9HlhTSgRERERiY4hlIiIiIhExxBKRERERKJjCCUiIiIi0TGEEhEREZHoGEKJiIiISHQMoUREREQkOobQYu7q1avw9PRE5cqVYWtriw4dOmDXrl1FXazPTmxsLNasWYM+ffqgbt26qFChAqpXr46hQ4fiypUrSrdJTEyEj48P6tatCysrK9StWxc+Pj5ITEwUufSftxUrVkAqlUIqleLy5ctK1+G1KjoHDx5E79694eTkhIoVK6J+/foYMWIEYmJicq3Ha1Q05HI5Dhw4gB49eqBGjRqwsbFBkyZNMHHiRDx+/DjP+rxOhWvHjh2YOHEi2rVrBysrK0ilUvj5+alcX5vrsWvXLnTo0AG2traoXLkyPD09ce3atcI4HQ5WX5yFhISgX79+MDQ0RN++fWFhYYGDBw8iKioKs2fPxuTJk4u6iJ+NOXPm4Ndff4WTkxNatmyJChUqIDw8HIcPH4ZcLsf69evRp08fYf2UlBR07doVt27dQvv27eHi4oLbt2/jxIkTqFevHo4dOwZTU9MiPKPPw/3799GmTRsYGBggJSUFx48fxxdffJFrHV6roiGXyzFp0iRs2rQJTk5O6NixI8zMzBAXF4dz587hzz//RIsWLQDwGhWlWbNmYfXq1ahYsSK6d+8Oc3Nz3L59G8HBwTAzM0NgYCBq164NgNdJDPXq1cOTJ09gaWkJExMTPHnyBKtXr8bgwYPzrKvN9Vi6dCl+/vln2NnZwcPDAykpKdizZw9SU1Oxe/dutG7dWqfnwxBaTGVmZuKLL75AbGwsgoKC4OLiAgBISkqCm5sbHj58iIsXL8LZ2bmIS/p5OHDgAMqXLw9XV9dcj4eFhcHDwwNmZma4d+8ejIyMAADz58/HokWLMGHCBMydO1dYX/H4tGnT4OPjI+o5fG6ysrLQuXNnSCQSODs7Y+fOnUpDKK9V0fj9998xY8YMfPvtt1i4cGGe6QQzMzNhYGAAgNeoqDx//hy1atWCnZ0dQkNDYWFhISxbs2YNfHx8MHjwYKxevRoAr5MYTp8+jSpVqsDBwQHLly/H3LlzVYZQTa9HeHg4mjVrBkdHR5w8eRJlypQBANy9excdO3aEtbU1Ll++LLwvdYG344ups2fPIjIyEv379xcCKACYm5tj6tSpyMzMzLcKnnSrV69eeQIoALi6uqJ169Z48+YN7ty5AyC7hmfr1q0wMzPDtGnTcq3//fffQyqVYtu2bZDL+fuvMP3666+4ffs2Vq1apXK+ZF6rovH+/Xv4+vrC0dERCxYsUHp9FF90vEZFJzo6GjKZDM2bN88VQAGgS5cuAICXL18C4HUSS7t27eDg4FDgetpcDz8/P2RmZmLy5MlCAAWAWrVqYeDAgYiMjMTZs2d1dzJgCC22QkNDAQAdOnTIs0zx2Llz50QtEylXqlQpABC+SMPDwxEXF4dmzZrludVhbGwMV1dXxMbGIiIiQvSyfi7u3LkDX19fTJkyBbVq1VK5Hq9V0Th16hTevHkDd3d3ZGVl4cCBA1i+fDk2bNiQ57nmNSo6zs7OMDQ0xIULF5CUlJRrWVBQEAAIt2d5nYoXba5HUeQO3dWpkk6Fh4cDgNLb7VKpFJaWlsI6VHSePHmC06dPw9raGnXq1AHwv2tXpUoVpdsorml4eDibUxSCzMxMjB49GtWrV8ekSZPyXZfXqmgoOjkYGBigVatWePjwobBMT08Po0ePxrx58wDwGhWlcuXKYfbs2Zg9ezaaNWuGbt26wczMDHfu3MHp06cxfPhwfPfddwB4nYobba5HeHg4zMzMYG1tne/6usQQWkwpeq59eAtEwdzcHLGxsWIWiT6QkZGB7777DmlpaZg7d65QE6q4djlvZ+Rkbm6eaz3SraVLlwqN7xW11KrwWhUNxS3cVatWwcXFBcHBwahevTpu3ryJiRMnYtWqVXBycsKIESN4jYrYuHHjULFiRUyaNAnr168XHm/WrBm8vLyE9xivU/GizfVITExEhQoV1F5fF3g7nkgLMpkMY8aMQVhYGL788ksMHDiwqItEAG7duoUlS5Zg3LhxaNCgQVEXh1SQyWQAAENDQ/j5+aFRo0YwMzODq6srNm/eDD09PaxataqIS0kAsHjxYowePRqTJk3Cv//+i6dPn+LYsWPIzMxEz549ceDAgaIuIpVgDKHFlKIGVNWvjqSkJJW1pFS45HI5xo8fj507d8LLywvLly/PtVxxXd6+fat0e0XbKl4/3Rs1ahScnJwwY8YMtdbntSoaiuezQYMGsLGxybWsVq1acHR0RGRkJBISEniNitCZM2fwyy+/4Ntvv8XkyZNRqVIlmJqaonnz5tixYwdKly4t9K7mdSpetLkeFhYW+WaOD9fXBYbQYiq/9hcJCQl49eoV29UUAZlMhrFjx2Lbtm3o378/1q5dCz293G8jxXVR1QA/v/a+9HFu376NBw8ewNraWhigXiqVwt/fHwDQuXNnSKVSHDp0CACvVVGpVq0aANW3ChWPp6am8hoVoQ87H+VUvnx51K5dGzExMbm+j3idigdtroezszOSk5Px/PlztdbXBbYJLaZatmyJZcuWITg4GP369cu1LDg4WFiHxCOTyTBu3Dj4+fmhb9+++OOPP5QOLePs7AwbGxtcvHgRKSkpuXompqamIiwsDDY2NiobjJP2hg4dqvTxsLAwhIeHo1u3bihfvrwwxAmvVdFQhJoHDx7kWZaRkYGIiAiYmpqifPnysLa25jUqIunp6QD+14b3Q4rHDQ0N+V4qZrS5Hi1btsSlS5cQHBwMb2/vXPsrrNzBmtBiqm3btnB0dMTff/+NmzdvCo8nJSVh8eLFMDAwwKBBg4qwhJ8XRQ2on58fevfujXXr1qkce1IikWDo0KFITk7GokWLci1btmwZEhISMHToUEgkEjGK/llZuXKl0r+mTZsCyB4fb+XKlahfvz4AXqui4uTkhA4dOiAiIgJbtmzJtWz58uV4+/Yt3N3dYWBgwGtUhJo3bw4ge2D6D2/rbt++HREREWjQoAHMzc15nYoZba7H4MGDYWBggKVLl+a63nfv3kVAQACcnJzQpk0b3ZaTMyYVX2fPnkW/fv1gZGSEfv36wdzcXJi284cffsCUKVOKuoifjQULFsDX1xdmZmYYOXKk0gDq7u4uhJsPp0tr0KABbt++jePHj3P6uiIwatQo+Pv7qzVtJ6+VOCIjI+Hm5ob4+Hh06dIF1apVw82bN3H27FnY29vjxIkTwlAxvEZFIysrCx4eHggNDUX58uXRrVs3SKVS3L59G6dOnYKRkRH27duncnpVXifd27JlC86fPw8gezzkGzduoHnz5nBycgKQ/T3Uo0cPANpdjyVLlmDevHnCtJ3v3r3D7t278f79e+zevZsh9HPzzz//YMGCBbh06RIyMjJQs2ZNjBo1Cl5eXkVdtM+KIsTk58Op096+fQtfX18cOHAAz58/h7W1NXr16oXp06erbAtHhSO/EArwWhWVmJgYzJ8/HydPnsTr169hbW2Nbt26Ydq0aXmGiuE1KhppaWn4/fffsWfPHjx8+BDp6emwsrJCy5YtMWnSJGHeeAVep8JV0HfR9OnTMXPmTOH/2lyPnTt3Yu3atbh37x5KlSqFpk2bwsfHB40aNdL5+TCEEhEREZHo2CaUiIiIiETHEEpEREREomMIJSIiIiLRMYQSERERkegYQomIiIhIdAyhRERERCQ6hlAiIiIiEh1DKBERERGJjiGUiIiIiETHEEpEREREojMo6gIQERU1d3d3nDt3Ltdjenp6sLCwQPXq1eHu7o5vvvkGpqamRVTC4icqKgrbt29HmTJlMHr06KIuDhGVQJw7nog+e4oQamdnBzs7OwBARkYGHj9+jFevXgEAnJ2dcejQIdjY2BRlUYuNkJAQ9OzZE/b29rh161ZRF4eISiDejici+n+DBw/GsWPHcOzYMZw8eRLh4eHYvHkzTE1NER4eju+//76oi0hE9MlgCCUiyoeHhwemTp0KAAgMDMSbN2+KuERERJ8GhlAiogK0bdsWACCTyRAZGSk8fv78eXz99deoXbs2rKys4OjoiN69e2P//v1K9+Pn5wepVAp3d3dkZWXh999/R7t27WBvbw+pVIqEhARh3YSEBPj6+qJ9+/ZwcHBAxYoV0aBBAwwdOhT79u1Tuv+oqChMnToVTZo0gY2NDezs7NCuXTusXLkSqampSteXSqWQSqXC+Xh5ecHJyQkVK1aEq6sr1q1bB7k8d6std3d39OzZEwDw5MkTYR+Kv5CQEGHdsLAwzJ49Gx06dECNGjVQoUIFVKtWDV5eXjh27Fi+z/vz58/x/fffo06dOrC2tka9evUwY8YMJCQkYMGCBZBKpRg1apTSbdPS0rBu3Tp069YNjo6OsLKyQr169TBu3DhERETke1wiEgc7JhERFeDDEAYAc+bMwa+//goAsLCwQI0aNfDixQucPn0ap0+fxtdff41ly5ap3N/QoUNx5MgR2Nvbo1q1armC0T///INBgwbh+fPnALLbo5qbmyMmJgYHDx7EmTNn0Lt371z7PHjwIP7zn//g/fv3MDY2hqOjI9LS0nDz5k1cv34d+/fvx549e2BhYaG0TH5+fhg3bhzKlCkDR0dHREdH486dO5g2bRqio6Mxb948Yd3atWvjzZs3uHPnDoyMjNCwYcNc+8p5jCFDhuD169coW7YsKlasiIoVK+Lp06cICgpCUFAQpkyZgh9++CFPeSIiIuDu7o64uDjo6+ujZs2akMvlWLduHYKCguDm5qb0PADg2bNn8PLyws2bNyGRSGBraws7OztERERg69at2Lt3L/z8/IQfF0RUNFgTSkRUgLNnzwLI7jFfpUoVrF+/Hr/++issLS2xceNGREdHIyQkBPfv38fevXtRoUIFbNiwAX5+fkr3d/HiRZw/fx779+/HrVu3EBwcjAcPHsDCwgLPnz/HwIED8fz5c7Rt2xbXr1/HP//8g9OnT+PRo0e4evUqxo0bl2t/N2/exDfffIPU1FT88MMPiIyMxIULF3Dt2jVcuXIFjRo1wpUrVzB9+nSV5/j9999j3rx5ePToEU6dOoXw8HAhHK5evTpXDfDixYvh6+sLALCyshLa0Sr+XFxchHXnzJmD69evIzIyEufPn8eZM2fw6NEj7Nu3DxUqVMCSJUvwzz//5CnPf/7zH8TFxaFBgwa4du0azp07h7CwMFy5cgXGxsZYv3690vOQyWQYNmwYbt68ibZt2+Ly5cv4999/ERoaisePH2PSpElITk7GV199hdevX6t8Poio8DGEEhHlY//+/Vi8eDEAoEuXLjA0NMT8+fMBAOvWrUOfPn1yrd++fXssXboUAISa0g9lZWVh6dKluWriDA0Noaenh99++w3x8fGoWbMmduzYAUdHx1zbVqlSRWijqvDzzz8jLS0NEydOxJQpU1C6dOlc62/ZsgWmpqbYuXMnYmNjlZbJy8sLo0ePhr6+vvDY5MmTUbt2bcjlcgQGBubzLKk2bNiwPOcAAO3atcPs2bMBANu3b8+1LCQkBFeuXIGRkRG2bt0KBweHXOezefNmyGQypcfbv38/Ll26hCpVqsDPzw9Vq1YVlhkaGuLHH39E165d8fr1a2zevFmrcyIi3eDteCKi/+fn54czZ84AUD5E07JlyxASEoJXr17B3t4eHTt2VLqfbt26oVSpUnj48CHi4uLyDOtkYWGBXr16Kd32wIEDAIAxY8bA2Ni4wDInJiYiODgYADB8+HCl69jZ2aFhw4YIDQ3FuXPn4OnpmWedb775Js9jEokETZs2xZ07dz6qHeW9e/ewb98+/Pvvv3jz5g0yMzOFsgPAjRs3cq1/4sQJABDay36oWrVqaN68eZ6xXQEI7WU9PT1hZmamtDy9evXCsWPHcPbsWUyaNEnr8yKij8MQSkT0/2JiYhATEwMg+9a7ubk5mjZtmmuwekWtXWJiIrp27apyXxKJBAAQGxubJ4Q6OzvDwCDvx29SUhKePHkCAGjWrJlaZb579y6ysrIgkUjw3XffqVzv0aNHQnmUyVljmFOFChUAACkpKWqV50Nz5szBihUrlLarVfjwtvjDhw8BAPXq1VO5Td26dZWG0Nu3bwMA9uzZI/yg+NDbt28BqH4uiEgcDKFERP9v+vTpmDlzZr7rKHqwv337FhcuXChwn+/evcvzmImJidJ1k5KShH+XKVOmwH3nLI9cLte6PABUzgalp5fdakvV7e/87N69G7/++iv09PQwbdo09OzZE5UrV4apqSn09PRw5swZeHh4ICMjI9d2isCrqiYTAMzNzZU+rng+Hj58KIRZVVQ9F0QkDoZQIiINKMKau7u7yo5H2soZrN6+fQtra2u1y1OmTBlERUXptDwfS1FrPHbsWKXhXtWYq4pzSk5OVrnvnIH9w21fvXoFPz8/uLu7a1pkIhIROyYREWmgdu3aAIArV65oVTuYH3Nzc6EN5MWLF9XaplatWpBIJHj79i3u3bun0/LkR9HcID+KUNyiRQuly1WdY7Vq1QAg3+lAFbfdP6S4Puo+f0RUdBhCiYg00L59e5QpUwbPnz8vlN7VHh4eAIA1a9YgLS2twPUtLS3Rpk0bABB68YtB0aRA2SD4Cope+s+ePcuzLD4+Hv7+/kq3U3T4On36tNBGNqdHjx6pbHqgGK1gy5YtSo9LRMUHQygRkQbMzc2FoYWmT5+O1atX4/3797nWSUhIQEBAgLCeJsaPH48KFSrg7t278Pb2xuPHj3Mtj4yMzBM2586dC2NjY+zevRvjxo0TBrlXSE9Px8mTJ/Hll19qXB5VnJycIJFIEB8fj7t37ypdp2XLlgCApUuX4sGDB8Ljjx8/hpeXl8oA27p1azRp0gRpaWkYNmxYriAaGRmJYcOGCW1VP9S/f3988cUXSEhIQK9evXD+/Pk869y/fx+//PILjh49qvb5EpHusU0oEZGGvvnmG7x+/RoLFizArFmz8PPPP6NatWowNDTEy5cvER0dDblcLoQwTVhZWSEgIAADBw5EcHAwGjZsiKpVq8LMzAwxMTGIj4+HhYVFrrFCGzRogC1btuCbb77B1q1bhfExy5Qpg8TEREREROTp/POxypYtCzc3NwQGBqJt27aoVauW0JFowYIFqF+/PiZMmIA9e/YgJiYGrq6uqFq1KvT09HDv3j2Ym5vjp59+wrRp0/LsWyKRYN26dejevTuuXbuGBg0aoFatWpDJZLh37x4qV66Mr7/+Gn/88UeucU0BQF9fH9u3b8eQIUNw8eJFdOvWDVZWVrC3t0dGRgaePHkitEVdvXq1Tp8TItIMa0KJiLQwbdo0nD17FsOGDYOtrS0ePXqEe/fuoVSpUujUqRMWLVqEdevWabXvxo0b4+LFi5g6dSrq1KmDuLg43Lt3D6ampvDw8MCqVavybOPm5obLly9j8uTJqFu3Lp49e4abN2/i3bt3aNKkCaZPny7M/KQrf/zxB7799lvY2tri7t27OHfuHM6dOycMgWRjY4MTJ07Ay8sLUqkU4eHhSExMhLe3N86ePYtatWqp3HeVKlVw5swZfPXVV7CyssKDBw+QmJiIESNGIDg4GKVKlQKgvJd8hQoVcPjwYfzxxx/o3Lkz5HI5bt68idjYWNjZ2WHIkCHYvn07+vXrp9Png4g0I0lISFA9eBsREVEx5OXlhaCgICxcuBAjR44s6uIQkRZYE0pERCVKdHQ0Tp8+DQBwdXUt2sIQkdYYQomIqNiJjIzEqlWrhGlTFW7duoWBAwciPT0dLVq0QP369YuohET0sXg7noiIip2bN2+iTZs2kEgksLGxQcWKFYVOXwBgZ2eHgwcPwsnJqYhLSkTaYgglIqJiJyEhAatXr8bp06cRHR2NN2/eoFSpUnByckLXrl0xevRolCtXrqiLSUQfgSGUiIiIiETHNqFEREREJDqGUCIiIiISHUMoEREREYmOIZSIiIiIRMcQSkRERESiYwglIiIiItExhBIRERGR6BhCiYiIiEh0/wdwvv9kmunr7gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = AGENT_EVALUATION(Stockfish_path, WHITE_PLAYER_POLICY, n_evaluations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready to Play Chess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The position of the black bishops in the beggining\n",
    "state[:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inital black pawns\n",
    "state[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial positions\n",
    "state[:,:,:12].sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[494,\n",
       " 501,\n",
       " 129,\n",
       " 136,\n",
       " 1095,\n",
       " 1022,\n",
       " 949,\n",
       " 876,\n",
       " 803,\n",
       " 730,\n",
       " 657,\n",
       " 584,\n",
       " 1096,\n",
       " 1023,\n",
       " 950,\n",
       " 877,\n",
       " 804,\n",
       " 731,\n",
       " 658,\n",
       " 585]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.legal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Move.from_uci('c2c3')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode which is the 730 action\n",
    "env.decode(730)\n",
    "# it is a pawn from c2 to c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we play that action\n",
    "next_state, reward, done, info = env.step(730)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LetÂ´s Check if the pawn action was done\n",
    "next_state[:,:,:12].sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## I ran until Agent_Evaluation and stopped. Now i will run this ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 119)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the state_size\n",
    "state_size = env.observation_space.shape\n",
    "\n",
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4672"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the nÂº of actions\n",
    "action_size = env.action_space.n\n",
    "\n",
    "action_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Game Screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to preprocess the state\n",
    "def preprocess_state(state):\n",
    "    # Convert the chess board state to a binary representation\n",
    "    binary_state = np.zeros(64, dtype=np.float32)\n",
    "\n",
    "    for i in range(64):\n",
    "        piece = state.piece_at(i)\n",
    "        if piece is not None:\n",
    "            piece_value = {'P': 1, 'N': 2, 'B': 3, 'R': 4, 'Q': 5, 'K': 6,\n",
    "                           'p': -1, 'n': -2, 'b': -3, 'r': -4, 'q': -5, 'k': -6}\n",
    "            binary_state[i] = piece_value[piece.symbol()]\n",
    "\n",
    "    return binary_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to preprocess the state\n",
    "def preprocess_state(state):\n",
    "    # Convert the chess board state to a binary representation\n",
    "    binary_state = np.zeros(64, dtype=np.float32)\n",
    "\n",
    "    for i in range(64):\n",
    "        piece = state[i]\n",
    "        if piece != 0:\n",
    "            piece_value = {'P': 1, 'N': 2, 'B': 3, 'R': 4, 'Q': 5, 'K': 6,\n",
    "                           'p': -1, 'n': -2, 'b': -3, 'r': -4, 'q': -5, 'k': -6}\n",
    "            binary_state[i] = piece_value[piece]\n",
    "\n",
    "    return binary_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        #define the state size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        #define the action size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        #define the replay buffer\n",
    "        self.replay_buffer = deque(maxlen=5000)\n",
    "        \n",
    "        #define the discount factor\n",
    "        self.gamma = 0.9  \n",
    "        \n",
    "        #define the epsilon value\n",
    "        self.epsilon = 0.8   \n",
    "        \n",
    "        #define the update rate at which we want to update the target network\n",
    "        self.update_rate = 1000    \n",
    "        \n",
    "        #define the main network\n",
    "        self.main_network = self.build_network()\n",
    "        \n",
    "        #define the target network\n",
    "        self.target_network = self.build_network()\n",
    "        \n",
    "        #copy the weights of the main network to the target network\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "\n",
    "    #Let's define a function called build_network which is essentially our DQN. \n",
    "\n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=4, padding='same', input_shape=self.state_size))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (4, 4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "\n",
    "\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "\n",
    "        return model\n",
    "\n",
    "    #We learned that we train DQN by randomly sampling a minibatch of transitions from the\n",
    "    #replay buffer. So, we define a function called store_transition which stores the transition information\n",
    "    #into the replay buffer\n",
    "\n",
    "    def store_transistion(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "\n",
    "    #We learned that in DQN, to take care of exploration-exploitation trade off, we select action\n",
    "    #using the epsilon-greedy policy. So, now we define the function called epsilon_greedy\n",
    "    #for selecting action using the epsilon-greedy policy.\n",
    "    \n",
    "    def epsilon_greedy(self, state):\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        \n",
    "        Q_values = self.main_network.predict(state)\n",
    "        \n",
    "        return np.argmax(Q_values[0])\n",
    "\n",
    "    \n",
    "    #train the network\n",
    "    def train(self, batch_size):\n",
    "        \n",
    "        #sample a mini batch of transition from the replay buffer\n",
    "        minibatch = random.sample(self.replay_buffer, batch_size)\n",
    "        \n",
    "        #compute the Q value using the target network\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if not done:\n",
    "                target_Q = (reward + self.gamma * np.amax(self.target_network.predict(next_state)))\n",
    "            else:\n",
    "                target_Q = reward\n",
    "                \n",
    "            #compute the Q value using the main network \n",
    "            Q_values = self.main_network.predict(state)\n",
    "            \n",
    "            Q_values[0][action] = target_Q\n",
    "            \n",
    "            #train the main network\n",
    "            self.main_network.fit(state, Q_values, epochs=1, verbose=0)\n",
    "            \n",
    "    #update the target network weights by copying from the main network\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 250\n",
    "num_timesteps = 10000\n",
    "batch_size = 4\n",
    "num_screens = 2     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m Return \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#preprocess the game screen\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#for each step in the episode\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_timesteps):\n\u001b[0;32m     15\u001b[0m     \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m#render the environment\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [19], line 8\u001b[0m, in \u001b[0;36mpreprocess_state\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m64\u001b[39m):\n\u001b[0;32m      7\u001b[0m     piece \u001b[38;5;241m=\u001b[39m state[i]\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpiece\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[0;32m      9\u001b[0m         piece_value \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m6\u001b[39m,\n\u001b[0;32m     10\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m}\n\u001b[0;32m     11\u001b[0m         binary_state[i] \u001b[38;5;241m=\u001b[39m piece_value[piece]\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "time_step = 0\n",
    "\n",
    "#for each episode\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    #set return to 0\n",
    "    Return = 0\n",
    "    \n",
    "    #preprocess the game screen\n",
    "    state = preprocess_state(env.reset())\n",
    "\n",
    "    #for each step in the episode\n",
    "    for t in range(num_timesteps):\n",
    "        \n",
    "        #render the environment\n",
    "        env.render()\n",
    "        \n",
    "        #update the time step\n",
    "        time_step += 1\n",
    "        \n",
    "        #update the target network\n",
    "        if time_step % dqn.update_rate == 0:\n",
    "            dqn.update_target_network()\n",
    "        \n",
    "        #select the action\n",
    "        action = dqn.epsilon_greedy(state)\n",
    "        \n",
    "        #perform the selected action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        #preprocess the next state\n",
    "        next_state = preprocess_state(next_state)\n",
    "        \n",
    "        #store the transition information\n",
    "        dqn.store_transistion(state, action, reward, next_state, done)\n",
    "        \n",
    "        #update current state to next state\n",
    "        state = next_state\n",
    "        \n",
    "        #update the return\n",
    "        Return += reward\n",
    "        \n",
    "        #if the episode is done then print the return\n",
    "        if done:\n",
    "            print('Episode: ',i, ',' 'Return', Return)\n",
    "            break\n",
    "            \n",
    "        #if the number of transistions in the replay buffer is greater than batch size\n",
    "        #then train the network\n",
    "        if len(dqn.replay_buffer) > batch_size:\n",
    "            dqn.train(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Illegal move g1b1 for board position rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m action \u001b[38;5;241m=\u001b[39m dqn\u001b[38;5;241m.\u001b[39mepsilon_greedy(state)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# perform the selected action\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# preprocess the next state\u001b[39;00m\n\u001b[0;32m     29\u001b[0m next_state \u001b[38;5;241m=\u001b[39m preprocess_state(next_state)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\core.py:292\u001b[0m, in \u001b[0;36mActionWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\core.py:268\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 268\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym_chess\\envs.py:98\u001b[0m, in \u001b[0;36mChess.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ready, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_board\u001b[38;5;241m.\u001b[39mlegal_moves:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIllegal move \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for board position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_board\u001b[38;5;241m.\u001b[39mfen()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m     )\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_board\u001b[38;5;241m.\u001b[39mpush(action)\n\u001b[0;32m    104\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observation()\n",
      "\u001b[1;31mValueError\u001b[0m: Illegal move g1b1 for board position rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "time_step = 0\n",
    "\n",
    "# for each episode\n",
    "for i in range(num_episodes):\n",
    "    # set return to 0\n",
    "    Return = 0\n",
    "\n",
    "    # preprocess the game screen\n",
    "    state = preprocess_state(env.reset())\n",
    "\n",
    "    # for each step in the episode\n",
    "    for t in range(num_timesteps):\n",
    "\n",
    "        # update the time step\n",
    "        time_step += 1\n",
    "\n",
    "        # update the target network\n",
    "        if time_step % dqn.update_rate == 0:\n",
    "            dqn.update_target_network()\n",
    "\n",
    "        # select the action\n",
    "        action = dqn.epsilon_greedy(state)\n",
    "\n",
    "        # perform the selected action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # preprocess the next state\n",
    "        next_state = preprocess_state(next_state)\n",
    "\n",
    "        # store the transition information\n",
    "        dqn.store_transistion(state, action, reward, next_state, done)\n",
    "\n",
    "        # update current state to next state\n",
    "        state = next_state\n",
    "\n",
    "        # update the return\n",
    "        Return += reward\n",
    "\n",
    "        # if the episode is done then print the return\n",
    "        if done:\n",
    "            print('Episode:', i, ', Return:', Return)\n",
    "            break\n",
    "\n",
    "        # if the number of transitions in the replay buffer is greater than batch size\n",
    "        # then train the network\n",
    "        if len(dqn.replay_buffer) > batch_size:\n",
    "            dqn.train(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state):\n",
    "    binary_state = np.zeros((8, 8), dtype=np.float32)\n",
    "\n",
    "    piece_mapping = {'P': 1, 'N': 2, 'B': 3, 'R': 4, 'Q': 5, 'K': 6,\n",
    "                     'p': -1, 'n': -2, 'b': -3, 'r': -4, 'q': -5, 'k': -6}\n",
    "\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            piece = state[i, j]\n",
    "            if np.any(piece != 0):\n",
    "                binary_state[i, j] = piece_mapping.get(piece.item(0), 0)\n",
    "\n",
    "    return binary_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[494,\n",
       " 501,\n",
       " 129,\n",
       " 136,\n",
       " 1095,\n",
       " 1022,\n",
       " 949,\n",
       " 876,\n",
       " 803,\n",
       " 730,\n",
       " 657,\n",
       " 584,\n",
       " 1096,\n",
       " 1023,\n",
       " 950,\n",
       " 877,\n",
       " 804,\n",
       " 731,\n",
       " 658,\n",
       " 585]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.legal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    ">>> move = chess.Move.from_uci('e2e4')\n",
    ">>> env.encode(move)\n",
    "877\n",
    ">>> env.encode(move) in env.legal_actions\n",
    "True\n",
    "\n",
    ">>> env.decode(877)\n",
    "Move.from_uci('e2e4')\n",
    "\n",
    ">>> import gym_chess\n",
    ">>> from gym_chess.alphazero import BoardEncoding\n",
    "\n",
    ">>> env = gym.make('Chess-v0')\n",
    ">>> env = BoardEncoding(env, history_length=4)\n",
    ">>> env = MyEsotericMoveEncoding(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## Q LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 119)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the state_size\n",
    "state_size = env.observation_space.shape\n",
    "\n",
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7616"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space_size =8*8*119\n",
    "\n",
    "state_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4672"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the nÂº of actions\n",
    "action_size = env.action_space.n\n",
    "\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "Q = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning agent policies\n",
    "def WHITE_PLAYER_POLICY(env, state, Q, epsilon):\n",
    "    legal_actions = env.legal_actions\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.choice(legal_actions)\n",
    "    else:\n",
    "        action = np.argmax(Q[state][legal_actions])\n",
    "\n",
    "    return action\n",
    "\n",
    "def BLACK_PLAYER_POLICY(env, state, Q, epsilon):\n",
    "    legal_actions = env.legal_actions\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.choice(legal_actions)\n",
    "    else:\n",
    "        action = np.argmax(Q[state][legal_actions])\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06590b3347e74cd0b72dc795318c4165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 494 is out of bounds for axis 0 with size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [61], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Run Q-learning\u001b[39;00m\n\u001b[0;32m     45\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m---> 46\u001b[0m rewards, Q \u001b[38;5;241m=\u001b[39m \u001b[43mq_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Plot rewards\u001b[39;00m\n\u001b[0;32m     49\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(rewards)\n",
      "Cell \u001b[1;32mIn [61], line 28\u001b[0m, in \u001b[0;36mq_learning\u001b[1;34m(env, Q, episodes)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m<\u001b[39m (episodes \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m---> 28\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[43mWHITE_PLAYER_POLICY\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m         action \u001b[38;5;241m=\u001b[39m BLACK_PLAYER_POLICY(env, state, Q, epsilon)\n",
      "Cell \u001b[1;32mIn [60], line 7\u001b[0m, in \u001b[0;36mWHITE_PLAYER_POLICY\u001b[1;34m(env, state, Q, epsilon)\u001b[0m\n\u001b[0;32m      5\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(legal_actions)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlegal_actions\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[1;31mIndexError\u001b[0m: index 494 is out of bounds for axis 0 with size 8"
     ]
    }
   ],
   "source": [
    "import chess\n",
    "import gym\n",
    "import gym_chess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Q-learning hyperparameters\n",
    "alpha = 0.5  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "\n",
    "# Q-learning algorithm\n",
    "def q_learning(env, Q, episodes):\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in tqdm(range(episodes)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            if episode < (episodes / 2):\n",
    "                action = WHITE_PLAYER_POLICY(env, state, Q, epsilon)\n",
    "            else:\n",
    "                action = BLACK_PLAYER_POLICY(env, state, Q, epsilon)\n",
    "                \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Q-learning update\n",
    "            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return rewards, Q\n",
    "\n",
    "# Run Q-learning\n",
    "episodes = 1000\n",
    "rewards, Q = q_learning(env, Q, episodes)\n",
    "\n",
    "# Plot rewards\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Q-Learning Rewards')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = 4672  # Number of possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        #define the state size\n",
    "        self.state_size = (86, 86, 1)\n",
    "        \n",
    "        #define the action size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        #define the replay buffer\n",
    "        self.replay_buffer = deque(maxlen=1000)\n",
    "        \n",
    "        #define the discount factor\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        #define the epsilon value\n",
    "        self.epsilon = 0.99\n",
    "        \n",
    "        #define the update rate at which we want to update the target network\n",
    "        self.update_rate = 5\n",
    "        \n",
    "        #define the main network\n",
    "        self.main_network = self.build_network()\n",
    "        \n",
    "        #define the target network\n",
    "        self.target_network = self.build_network()\n",
    "        \n",
    "        #copy the weights of the main network to the target network\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "\n",
    "        #learning rate\n",
    "        self.learning_rate = .0001\n",
    "        \n",
    "\n",
    "    #Let's define a function called build_network which is essentially our DQN. \n",
    "\n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        \"\"\"\n",
    "        model.add(Conv2D(8, (8, 8), strides=4, padding='same', input_shape=self.state_size))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(16, (4, 4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(16, (3, 3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "\n",
    "\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \"\"\"\n",
    "        \n",
    "        model.add(Conv2D(filters=6, kernel_size=(7, 7), strides=3, activation='relu', input_shape=self.state_size))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(filters=12, kernel_size=(4, 4), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(216, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation=None))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=.0001, epsilon=1e-7))\n",
    "\n",
    "        return model\n",
    "\n",
    "    #We learned that we train DQN by randomly sampling a minibatch of transitions from the\n",
    "    #replay buffer. So, we define a function called store_transition which stores the transition information\n",
    "    #into the replay buffer\n",
    "\n",
    "    def store_transistion(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "\n",
    "    #We learned that in DQN, to take care of exploration-exploitation trade off, we select action\n",
    "    #using the epsilon-greedy policy. So, now we define the function called epsilon_greedy\n",
    "    #for selecting action using the epsilon-greedy policy.\n",
    "    \n",
    "    def epsilon_greedy(self, state):\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            random_discrete_action = np.random.choice(list(range(12)))\n",
    "            \n",
    "            return random_discrete_action\n",
    "        \n",
    "        Q_values = self.main_network.predict(state, verbose=0)\n",
    "        \n",
    "        discrete_action = np.argmax(Q_values[0])\n",
    "        \n",
    "        return discrete_action\n",
    "\n",
    "    \n",
    "    #train the network\n",
    "    def train(self, batch_size):\n",
    "        \n",
    "        minibatch = np.array(random.sample(dqn.replay_buffer, batch_size), dtype=object)\n",
    "\n",
    "        state_list = np.array(minibatch[:,0], dtype=object)\n",
    "        state_list = np.hstack(state_list).reshape(batch_size, 86, 86, 1)\n",
    "\n",
    "        next_state_list = np.array(minibatch[:,3])\n",
    "        next_state_list = np.hstack(next_state_list).reshape(batch_size, 86, 86, 1)\n",
    "\n",
    "        current_Q_values_list = dqn.main_network.predict(state_list, verbose=0)\n",
    "\n",
    "        max_q = np.amax(dqn.target_network.predict(next_state_list, verbose=0), axis=1)\n",
    "\n",
    "        for i, zip_ in enumerate(minibatch):\n",
    "\n",
    "            state, action, reward, next_state, done = zip_\n",
    "\n",
    "            if not done:\n",
    "                target  = reward + dqn.gamma * max_q[i]\n",
    "            else:\n",
    "                target = reward\n",
    "\n",
    "            updated_Q_value = target # (1 - self.learning_rate)*current_Q_values_list[i][action] + self.learning_rate*(target) # - current_Q_values_list[i][action]) # This is a different form of Q-learning (Min Q-Learning)\n",
    "\n",
    "            current_Q_values_list[i][action] = updated_Q_value\n",
    "        \n",
    "\n",
    "        #train the main network\n",
    "        self.main_network.fit(state_list, current_Q_values_list, epochs=1, verbose=0)\n",
    "            \n",
    "    #update the target network weights by copying from the main network\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "Now, let's train the network. First, let's set the number of episodes we want to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "# Define the batch size:\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the DQN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 8\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 0\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 7\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 3\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 5\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 8\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 11\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 3\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 2\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 1\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 4\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 9\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 9\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 5\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 3\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 9\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 3\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 2\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 7\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 9\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 3\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 0\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 5\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 11\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 8\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 0\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 8\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 9\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 7\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 8\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 2\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 1\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 3\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 0\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 9\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 4\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 7\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 7\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 2\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 1\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 1\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 5\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 2\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 5\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 3\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 0\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 0\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 8\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 1\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 3\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 8\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 4\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 2\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 11\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 4\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 11\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 7\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 8\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 5\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 8\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 5\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 2\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 5\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 2\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 11\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 11\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 3\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 11\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 9\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 11\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 8\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 5\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 11\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 11\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 7\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 0\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 11\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 4\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 5\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 3\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 8\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 9\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 7\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 6\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n",
      "Chosen action is illegal: 10\n",
      "Legal Actions: ['g1h3', 'g1f3', 'b1c3', 'b1a3', 'h2h3', 'g2g3', 'f2f3', 'e2e3', 'd2d3', 'c2c3', 'b2b3', 'a2a3', 'h2h4', 'g2g4', 'f2f4', 'e2e4', 'd2d4', 'c2c4', 'b2b4', 'a2a4']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 86, 86, 1), found shape=(None, 8, 119)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [65], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLegal Actions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, legal_actions)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Choose an action using epsilon-greedy strategy\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon_greedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Check if the chosen action is in the list of legal actions\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m legal_moves:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Handle the case when the chosen action is illegal\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [28], line 85\u001b[0m, in \u001b[0;36mDQN.epsilon_greedy\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     81\u001b[0m     random_discrete_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m)))\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random_discrete_action\n\u001b[1;32m---> 85\u001b[0m Q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m discrete_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q_values[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m discrete_action\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filepuah72_v.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\David\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_11\" is incompatible with the layer: expected shape=(None, 86, 86, 1), found shape=(None, 8, 119)\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "    # Get the legal actions for the current state\n",
    "        legal_moves = env.legal_moves\n",
    "\n",
    "    # Convert legal_moves to a list of strings\n",
    "        legal_actions = [str(move) for move in legal_moves]\n",
    "\n",
    "    # Print the legal actions\n",
    "        print(\"Legal Actions:\", legal_actions)\n",
    "\n",
    "    # Choose an action using epsilon-greedy strategy\n",
    "        action = dqn.epsilon_greedy(state)\n",
    "\n",
    "    # Check if the chosen action is in the list of legal actions\n",
    "        if action not in legal_moves:\n",
    "        # Handle the case when the chosen action is illegal\n",
    "            print(\"Chosen action is illegal:\", action)\n",
    "            continue\n",
    "\n",
    "    # Take action in the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    # Store the transition in the replay buffer\n",
    "        dqn.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "    # Update the Q-network\n",
    "        dqn.update_network()\n",
    "\n",
    "    # Update the target network\n",
    "        dqn.update_target_network()\n",
    "\n",
    "    # Update the current state\n",
    "        state = next_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained DQN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = AGENT_EVALUATION(Stockfish_path, dqn.epsilon_greedy, n_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnvironment:\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "\n",
    "    def step(self, move):\n",
    "        # Apply the move to the board\n",
    "        self.board.push(move)\n",
    "\n",
    "        # Get the next state after applying the move\n",
    "        next_state = self.board.fen()\n",
    "\n",
    "        # Determine the reward based on the current state\n",
    "        reward = self.calculate_reward()\n",
    "\n",
    "        # Check if the game is terminated\n",
    "        done = self.board.is_game_over()\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        # Implement your reward calculation logic here\n",
    "        # Return the reward value based on the current state\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the replay memory buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones, dtype=np.uint8)\n",
    "\n",
    "        states = np.reshape(states, (batch_size, 8, 8, 12))\n",
    "        next_states = np.reshape(next_states, (batch_size, 8, 8, 12))\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Define the Deep Q-Network\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc1 = layers.Dense(64, activation=\"relu\")\n",
    "        self.fc2 = layers.Dense(num_actions)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the Deep Q-Learning agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, num_actions, replay_capacity=10000, batch_size=32, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.num_actions = num_actions\n",
    "        self.replay_buffer = ReplayBuffer(replay_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.model = DQN(num_actions)\n",
    "        self.target_model = DQN(num_actions)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        q_values = self.model(np.expand_dims(state, axis=0))\n",
    "        valid_moves = self.get_valid_moves(state)\n",
    "        valid_q_values = tf.gather(q_values[0], valid_moves)\n",
    "        return tf.argmax(valid_q_values).numpy()\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        chessboard = chess.Board(state)\n",
    "        valid_moves = []\n",
    "        for move in chessboard.legal_moves:\n",
    "            valid_moves.append(self.move_to_index(move))\n",
    "        return valid_moves\n",
    "\n",
    "    def move_to_index(self, move):\n",
    "        return move.from_square * 64 + move.to_square\n",
    "\n",
    "    def index_to_move(self, index):\n",
    "        from_square = index // 64\n",
    "        to_square = index % 64\n",
    "        return chess.Move(from_square, to_square)\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        next_states = np.reshape(next_states, (self.batch_size, 8, 8, 12))  # Reshape next_states\n",
    "\n",
    "        next_q_values = self.target_model.predict(next_states)\n",
    "        targets = rewards + (1 - dones) * self.gamma * np.amax(next_q_values, axis=1)\n",
    "\n",
    "        mask = tf.one_hot(actions, self.num_actions)\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(states)\n",
    "            q_action_values = tf.reduce_sum(tf.multiply(q_values, mask), axis=1)\n",
    "            loss = tf.reduce_mean(tf.square(targets - q_action_values))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Board' object has no attribute 'step'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [82], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(valid_moves):\n\u001b[0;32m     17\u001b[0m     move \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mindex_to_move(valid_moves[action])\n\u001b[1;32m---> 18\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m(move)\n\u001b[0;32m     19\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[0;32m     20\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Board' object has no attribute 'step'"
     ]
    }
   ],
   "source": [
    "# Initialize the DQNAgent and other variables\n",
    "env = chess.Board()\n",
    "num_actions = env.legal_moves.count()\n",
    "agent = DQNAgent(num_actions)\n",
    "episodes = 1000\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(episodes):\n",
    "    state = env.fen()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        valid_moves = agent.get_valid_moves(state)\n",
    "        if action < len(valid_moves):\n",
    "            move = agent.index_to_move(valid_moves[action])\n",
    "            next_state, reward, done = env.step(move)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        if len(agent.replay_buffer) >= agent.batch_size:\n",
    "            agent.train()\n",
    "            agent.update_target_model()\n",
    "\n",
    "    agent.train()\n",
    "    agent.update_target_model()\n",
    "\n",
    "    print(\"Episode:\", episode, \"Total Reward:\", total_reward)\n",
    "\n",
    "# Evaluate the agent's performance\n",
    "evaluation_episodes = 100\n",
    "total_wins = 0\n",
    "\n",
    "for _ in range(evaluation_episodes):\n",
    "    state = env.fen()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        valid_moves = agent.get_valid_moves(state)\n",
    "        if action < len(valid_moves):\n",
    "            move = agent.index_to_move(valid_moves[action])\n",
    "            env.push(move)\n",
    "            state = env.fen()\n",
    "            done = env.is_game_over()\n",
    "\n",
    "    if env.is_checkmate():\n",
    "        total_wins += 1\n",
    "\n",
    "win_rate = total_wins / evaluation_episodes\n",
    "print(\"Win Rate:\", win_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
